{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backend API Demo\n\nThis guide walks through manually testing some of the features of StreamWeave to demonstrate its capabilities: \n\n- Prefect pipeline orchestration\n- rclone data transfers\n- File transfer hook system\n- File visibility/access rules\n\nThe demo includes a \"simlab\", which consists of three simulated instruments with network shares that serve\nas the source data for the harvesting demo. Files are transferred into multiple docker volumes in this example,\nwhich represent potential transfer destination targets in a real deployment.\n\nThis page is a static rendering of a Jupyter Notebook, which you can <a href=\"./backend-demo.ipynb\" download>&#x2913; download </a> to run locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- Docker and Docker Compose installed\n",
    "- `uv` installed for Python package management\n",
    "- `git` installed to clone the `streamweave` repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the notebook, you'll need to ensure Jupyter is setup:\n",
    "\n",
    "> **Jupyter setup** (if needed):\n",
    "> ```bash\n",
    "> git clone https://github.com/datasophos/streamweave.git\n",
    "> cd backend\n",
    "> uv sync\n",
    "> uv run jupyter lab ../docs/backend-demo.ipynb\n",
    "> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell contains helper commands that will be used throughout the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import threading\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "from cryptography.fernet import Fernet\n",
    "\n",
    "# Find the repo root regardless of where Jupyter was launched from\n",
    "def _find_repo_root():\n",
    "    p = Path.cwd()\n",
    "    while p != p.parent:\n",
    "        if (p / \"docker-compose.yml\").exists():\n",
    "            return p\n",
    "        p = p.parent\n",
    "    raise RuntimeError(\"Could not find repo root (no docker-compose.yml found)\")\n",
    "\n",
    "REPO_ROOT = _find_repo_root()\n",
    "SIMLAB_COMPOSE = str(REPO_ROOT / \"simlab\" / \"docker-compose.simlab.yml\")\n",
    "\n",
    "# Generate a Fernet key for this session (used by docker-compose and seed.py)\n",
    "ENCRYPTION_KEY = Fernet.generate_key().decode()\n",
    "SECRET_KEY = Fernet.generate_key().decode()\n",
    "\n",
    "# Environment variables for docker-compose and scripts\n",
    "STREAMWEAVE_ENV = {\n",
    "    **os.environ,\n",
    "    \"STREAMWEAVE_ENCRYPTION_KEY\": ENCRYPTION_KEY,\n",
    "    \"SECRET_KEY\": SECRET_KEY,\n",
    "    \"DATABASE_URL\": \"postgresql+asyncpg://streamweave:streamweave@localhost:5432/streamweave\",\n",
    "}\n",
    "\n",
    "BASE_URL = \"http://localhost:8000\"\n",
    "PREFECT_API_URL = \"http://localhost:4200/api\"\n",
    "\n",
    "client = httpx.Client(base_url=BASE_URL, timeout=30)\n",
    "prefect = httpx.Client(base_url=PREFECT_API_URL, timeout=30)\n",
    "\n",
    "\n",
    "def pp(resp, n: int | None = None):\n",
    "    \"\"\"\n",
    "    Pretty-print a JSON response.\n",
    "    \n",
    "    Prints the first `n` items, if given as an argument.\n",
    "    \"\"\"\n",
    "    try:\n",
    "      data = resp.json()\n",
    "      if n is not None and isinstance(data, list) and len(data) > n:\n",
    "          data = [*data[:n], \"...\"]\n",
    "      print(json.dumps(data, indent=2))\n",
    "    except Exception:\n",
    "      print(f\"HTTP {resp.status_code}: {resp.text}\")\n",
    "\n",
    "\n",
    "def run(cmd, **kwargs):\n",
    "    \"\"\"Run a shell command, streaming stdout normally and stderr in yellow.\"\"\"\n",
    "    YELLOW = \"\\033[33m\"\n",
    "    RESET = \"\\033[0m\"\n",
    "    \n",
    "    # Use STREAMWEAVE_ENV by default if no env is specified\n",
    "    if \"env\" not in kwargs:\n",
    "        kwargs[\"env\"] = STREAMWEAVE_ENV\n",
    "    \n",
    "    with subprocess.Popen(\n",
    "      cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, **kwargs\n",
    "    ) as proc:\n",
    "      def _stream_stderr():\n",
    "          for line in proc.stderr:\n",
    "              print(f\"{YELLOW}{line}{RESET}\", end=\"\", flush=True)\n",
    "    \n",
    "      t = threading.Thread(target=_stream_stderr)\n",
    "      t.start()\n",
    "    \n",
    "      for line in proc.stdout:\n",
    "          print(line, end=\"\", flush=True)\n",
    "    \n",
    "      t.join()\n",
    "    \n",
    "    if proc.returncode != 0:\n",
    "      warnings.warn(f\"Command exited with code {proc.returncode}: {cmd}\")\n",
    "    \n",
    "    return proc\n",
    "\n",
    "\n",
    "def wait_for_flow_run(flow_run_id: str, timeout: int = 120) -> str:\n",
    "    \"\"\"\n",
    "    Wait for a Prefect flow run to complete.\n",
    "    \n",
    "    Args:\n",
    "        flow_run_id: The UUID of the flow run to wait for\n",
    "        timeout: Maximum seconds to wait (default 120)\n",
    "    \n",
    "    Returns:\n",
    "        The final state type (COMPLETED, FAILED, CANCELLED, CRASHED, or TIMEOUT)\n",
    "    \"\"\"\n",
    "    terminal_states = (\"COMPLETED\", \"FAILED\", \"CANCELLED\", \"CRASHED\")\n",
    "    \n",
    "    print(f\"Waiting for flow run {flow_run_id} to complete...\")\n",
    "    for attempt in range(timeout):\n",
    "        flow_run = prefect.get(f\"/flow_runs/{flow_run_id}\").json()\n",
    "        state = flow_run.get(\"state\", {}).get(\"type\", \"UNKNOWN\")\n",
    "        if state in terminal_states:\n",
    "            print(f\"Flow run finished with state: {state}\")\n",
    "            return state\n",
    "        print(f\"  State: {state} (attempt {attempt + 1}/{timeout})\")\n",
    "        time.sleep(1)\n",
    "    \n",
    "    print(f\"Warning: Flow run did not complete within {timeout} seconds\")\n",
    "    return \"TIMEOUT\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting the Stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a. Start the \"simlab\" stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m Container simlab-xray-diffraction-01-1 Stopping \n",
      "\u001b[0m\u001b[33m Container simlab-microscope-01-1 Stopping \n",
      "\u001b[0m\u001b[33m Container simlab-spectrometer-01-1 Stopping \n",
      "\u001b[0m\u001b[33m Container simlab-spectrometer-01-1 Stopped \n",
      "\u001b[0m\u001b[33m Container simlab-spectrometer-01-1 Removing \n",
      "\u001b[0m\u001b[33m Container simlab-microscope-01-1 Stopped \n",
      "\u001b[0m\u001b[33m Container simlab-microscope-01-1 Removing \n",
      "\u001b[0m\u001b[33m Container simlab-spectrometer-01-1 Removed \n",
      "\u001b[0m\u001b[33m Container simlab-xray-diffraction-01-1 Stopped \n",
      "\u001b[0m\u001b[33m Container simlab-xray-diffraction-01-1 Removing \n",
      "\u001b[0m\u001b[33m Container simlab-microscope-01-1 Removed \n",
      "\u001b[0m\u001b[33m Container simlab-xray-diffraction-01-1 Removed \n",
      "\u001b[0m\u001b[33m Network streamweave-simlab Removing \n",
      "\u001b[0m\u001b[33m Network streamweave-simlab Resource is still in use \n",
      "\u001b[0m\u001b[33m Image simlab-xray-diffraction-01 Building \n",
      "\u001b[0m\u001b[33m Image simlab-microscope-01 Building \n",
      "\u001b[0m\u001b[33m Image simlab-spectrometer-01 Building \n",
      "\u001b[0m#1 [internal] load local bake definitions\n",
      "#1 reading from stdin 1.62kB done\n",
      "#1 DONE 0.0s\n",
      "\n",
      "#2 [microscope-01 internal] load build definition from Dockerfile.samba\n",
      "#2 transferring dockerfile: 90B done\n",
      "#2 DONE 0.0s\n",
      "\n",
      "#3 [microscope-01 internal] load metadata for docker.io/dperson/samba:latest\n",
      "#3 DONE 0.0s\n",
      "\n",
      "#4 [xray-diffraction-01 internal] load .dockerignore\n",
      "#4 transferring context: 2B done\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#5 [xray-diffraction-01 1/2] FROM docker.io/dperson/samba:latest@sha256:66088b78a19810dd1457a8f39340e95e663c728083efa5fe7dc0d40b2478e869\n",
      "#5 resolve docker.io/dperson/samba:latest@sha256:66088b78a19810dd1457a8f39340e95e663c728083efa5fe7dc0d40b2478e869 done\n",
      "#5 DONE 0.0s\n",
      "\n",
      "#6 [spectrometer-01 2/2] RUN apk add --no-cache tree\n",
      "#6 CACHED\n",
      "\n",
      "#7 [spectrometer-01] exporting to image\n",
      "#7 exporting layers done\n",
      "#7 exporting manifest sha256:341ccbe7fc24ec6e63520c071304a94514a3e9a693860be04e425cc01b691e95 done\n",
      "#7 exporting config sha256:b88ae1c84b651d317491ae3f88ee75e595f33c152389c641fac7a6e7a0df3001 done\n",
      "#7 exporting attestation manifest sha256:5bd118a03e72741f81ad2e51b8da9578d1d4395b26bf8fb5a7a2492400ea76f4 0.0s done\n",
      "#7 exporting manifest list sha256:c6ce43dd2e8560ac5a991aa5ae080d89dbdee234626c82a7dc6c10bab6852444 done\n",
      "#7 naming to docker.io/library/simlab-spectrometer-01:latest 0.0s done\n",
      "#7 unpacking to docker.io/library/simlab-spectrometer-01:latest\n",
      "#7 unpacking to docker.io/library/simlab-spectrometer-01:latest done\n",
      "#7 DONE 0.1s\n",
      "\n",
      "#8 [microscope-01] exporting to image\n",
      "#8 exporting layers done\n",
      "#8 exporting manifest sha256:c0699bc3dc6bddd8a23a97b0490fcaa63ad803c82e53534af6062c88abf9b757 done\n",
      "#8 exporting config sha256:008b4c2f96cbe5ae248f26a2c7641453642d7e4a8a2944b424f25516eef1adbc done\n",
      "#8 exporting attestation manifest sha256:5491ad97c2b20fc2e936970c0100301589cb21fee56e1f89f05ccd13c089289a 0.0s done\n",
      "#8 exporting manifest list sha256:90a8269fdd2d9764d3512021b52f66e5172e54e0e3cf7036f03aa8eadea3a80a 0.0s done\n",
      "#8 naming to docker.io/library/simlab-microscope-01:latest done\n",
      "#8 unpacking to docker.io/library/simlab-microscope-01:latest 0.0s done\n",
      "#8 DONE 0.1s\n",
      "\n",
      "#9 [xray-diffraction-01] exporting to image\n",
      "#9 exporting layers done\n",
      "#9 exporting manifest sha256:30509964e385ece6a9b834da1947cc2e15aeea6094d176dab50d420ea28d4678 done\n",
      "#9 exporting config sha256:c7877cc919e0f10bb91d475c335d546e9e9fe7edf264340f068136c8696446a3 done\n",
      "#9 exporting attestation manifest sha256:74072703e150f23dbc9a4fafc881267a272f4de2c85d65f1b700c8d3058b120b 0.0s done\n",
      "#9 exporting manifest list sha256:01e8b61a13e25f391b75212304869bbaa5bed70f99e1a176f5edad7c2ff3f760 0.0s done\n",
      "#9 naming to docker.io/library/simlab-xray-diffraction-01:latest done\n",
      "#9 unpacking to docker.io/library/simlab-xray-diffraction-01:latest done\n",
      "#9 DONE 0.1s\n",
      "\n",
      "#10 [spectrometer-01] resolving provenance for metadata file\n",
      "#10 DONE 0.0s\n",
      "\u001b[33m Image simlab-spectrometer-01 Built \n",
      "\u001b[0m\n",
      "#11 [microscope-01] resolving provenance for metadata file\n",
      "\u001b[33m Image simlab-xray-diffraction-01 Built \n",
      "\u001b[0m#11 DONE 0.0s\n",
      "\u001b[33m Image simlab-microscope-01 Built \n",
      "\u001b[0m\n",
      "#12 [xray-diffraction-01] resolving provenance for metadata file\n",
      "#12 DONE 0.0s\n",
      "\u001b[33m Container simlab-xray-diffraction-01-1 Creating \n",
      "\u001b[0m\u001b[33m Container simlab-spectrometer-01-1 Creating \n",
      "\u001b[0m\u001b[33m Container simlab-microscope-01-1 Creating \n",
      "\u001b[0m\u001b[33m Container simlab-xray-diffraction-01-1 Created \n",
      "\u001b[0m\u001b[33m Container simlab-spectrometer-01-1 Created \n",
      "\u001b[0m\u001b[33m Container simlab-microscope-01-1 Created \n",
      "\u001b[0m\u001b[33m Container simlab-xray-diffraction-01-1 Starting \n",
      "\u001b[0m\u001b[33m Container simlab-microscope-01-1 Starting \n",
      "\u001b[0m\u001b[33m Container simlab-spectrometer-01-1 Starting \n",
      "\u001b[0m\u001b[33m Container simlab-microscope-01-1 Started \n",
      "\u001b[0m\u001b[33m Container simlab-xray-diffraction-01-1 Started \n",
      "\u001b[0m\u001b[33m Container simlab-spectrometer-01-1 Started \n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "_ = run(f\"env COMPOSE_FILE={SIMLAB_COMPOSE} sh -c 'docker compose down -v && docker compose up -d --build'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                           IMAGE                        COMMAND                  SERVICE               CREATED        STATUS                                     PORTS\n",
      "simlab-microscope-01-1         simlab-microscope-01         \"/sbin/tini -- /usr/\u2026\"   microscope-01         1 second ago   Up Less than a second (health: starting)   0.0.0.0:4451->445/tcp, [::]:4451->445/tcp\n",
      "simlab-spectrometer-01-1       simlab-spectrometer-01       \"/sbin/tini -- /usr/\u2026\"   spectrometer-01       1 second ago   Up Less than a second (health: starting)   0.0.0.0:4452->445/tcp, [::]:4452->445/tcp\n",
      "simlab-xray-diffraction-01-1   simlab-xray-diffraction-01   \"/sbin/tini -- /usr/\u2026\"   xray-diffraction-01   1 second ago   Up Less than a second (health: starting)   0.0.0.0:4453->445/tcp, [::]:4453->445/tcp\n"
     ]
    }
   ],
   "source": [
    "# remove any files that were created in previous iterations of running this notebook:\n",
    "_ = run(f\"docker compose -f {SIMLAB_COMPOSE} exec microscope-01 sh -c 'rm -rf /data/user_a /data/user_b /data/scratch.tmp'\")\n",
    "\n",
    "# print the currently running containers\n",
    "_ = run(f\"docker compose -f {SIMLAB_COMPOSE} ps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see three healthy containers: `microscope-01`, `spectrometer-01`, `xray-diffraction-01`. These containers each contain a [samba](https://www.samba.org/) share that represents real data produced on instrument computers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. Start the main stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of this command will clear any data you have in the local stack. Comment out the `docker compose down` part if you do not wish to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m Container streamweave-worker-1 Stopping \n",
      "\u001b[0m\u001b[33m Container streamweave-api-1 Stopping \n",
      "\u001b[0m\u001b[33m Container streamweave-worker-1 Stopped \n",
      "\u001b[0m\u001b[33m Container streamweave-worker-1 Removing \n",
      "\u001b[0m\u001b[33m Container streamweave-worker-1 Removed \n",
      "\u001b[0m\u001b[33m Container streamweave-api-1 Stopped \n",
      "\u001b[0m\u001b[33m Container streamweave-api-1 Removing \n",
      "\u001b[0m\u001b[33m Container streamweave-api-1 Removed \n",
      "\u001b[0m\u001b[33m Container streamweave-postgres-1 Stopping \n",
      "\u001b[0m\u001b[33m Container streamweave-prefect-server-1 Stopping \n",
      "\u001b[0m\u001b[33m Container streamweave-postgres-1 Stopped \n",
      "\u001b[0m\u001b[33m Container streamweave-postgres-1 Removing \n",
      "\u001b[0m\u001b[33m Container streamweave-postgres-1 Removed \n",
      "\u001b[0m\u001b[33m Container streamweave-prefect-server-1 Stopped \n",
      "\u001b[0m\u001b[33m Container streamweave-prefect-server-1 Removing \n",
      "\u001b[0m\u001b[33m Container streamweave-prefect-server-1 Removed \n",
      "\u001b[0m\u001b[33m Container streamweave-redis-1 Stopping \n",
      "\u001b[0m\u001b[33m Container streamweave-prefect-postgres-1 Stopping \n",
      "\u001b[0m\u001b[33m Container streamweave-prefect-postgres-1 Stopped \n",
      "\u001b[0m\u001b[33m Container streamweave-prefect-postgres-1 Removing \n",
      "\u001b[0m\u001b[33m Container streamweave-prefect-postgres-1 Removed \n",
      "\u001b[0m\u001b[33m Container streamweave-redis-1 Stopped \n",
      "\u001b[0m\u001b[33m Container streamweave-redis-1 Removing \n",
      "\u001b[0m\u001b[33m Container streamweave-redis-1 Removed \n",
      "\u001b[0m\u001b[33m Volume streamweave_pgdata Removing \n",
      "\u001b[0m\u001b[33m Volume streamweave_harvest_data Removing \n",
      "\u001b[0m\u001b[33m Volume streamweave_prefect_pgdata Removing \n",
      "\u001b[0m\u001b[33m Network streamweave_default Removing \n",
      "\u001b[0m\u001b[33m Volume streamweave_harvest_data Removed \n",
      "\u001b[0m\u001b[33m Volume streamweave_prefect_pgdata Removed \n",
      "\u001b[0m\u001b[33m Volume streamweave_pgdata Removed \n",
      "\u001b[0m\u001b[33m Network streamweave_default Removed \n",
      "\u001b[0m\u001b[33m Image streamweave-worker Building \n",
      "\u001b[0m\u001b[33m Image streamweave-api Building \n",
      "\u001b[0m#1 [internal] load local bake definitions\n",
      "#1 reading from stdin 1.04kB done\n",
      "#1 DONE 0.0s\n",
      "\n",
      "#2 [worker internal] load build definition from Dockerfile.worker\n",
      "#2 transferring dockerfile: 494B done\n",
      "#2 DONE 0.0s\n",
      "\n",
      "#3 [api internal] load build definition from Dockerfile\n",
      "#3 transferring dockerfile: 445B done\n",
      "#3 DONE 0.0s\n",
      "\n",
      "#4 [api internal] load metadata for docker.io/library/python:3.12-slim\n",
      "#4 DONE 0.4s\n",
      "\n",
      "#5 [worker internal] load .dockerignore\n",
      "#5 transferring context: 2B done\n",
      "#5 DONE 0.0s\n",
      "\n",
      "#6 [api internal] load build context\n",
      "#6 transferring context: 7.97kB done\n",
      "#6 DONE 0.0s\n",
      "\n",
      "#7 [worker 1/8] FROM docker.io/library/python:3.12-slim@sha256:9e01bf1ae5db7649a236da7be1e94ffbbbdd7a93f867dd0d8d5720d9e1f89fab\n",
      "#7 resolve docker.io/library/python:3.12-slim@sha256:9e01bf1ae5db7649a236da7be1e94ffbbbdd7a93f867dd0d8d5720d9e1f89fab 0.0s done\n",
      "#7 DONE 0.0s\n",
      "\n",
      "#8 [api 3/9] RUN apt-get update && apt-get install -y --no-install-recommends     gcc libpq-dev tree     && rm -rf /var/lib/apt/lists/*\n",
      "#8 CACHED\n",
      "\n",
      "#9 [api 6/9] COPY app/ app/\n",
      "#9 CACHED\n",
      "\n",
      "#10 [api 4/9] RUN pip install --no-cache-dir uv\n",
      "#10 CACHED\n",
      "\n",
      "#11 [api 8/9] COPY alembic.ini .\n",
      "#11 CACHED\n",
      "\n",
      "#12 [api 5/9] COPY pyproject.toml .\n",
      "#12 CACHED\n",
      "\n",
      "#13 [api 7/9] RUN uv pip install --system --no-cache .\n",
      "#13 CACHED\n",
      "\n",
      "#14 [api 9/9] COPY alembic/ alembic/\n",
      "#14 CACHED\n",
      "\n",
      "#15 [worker internal] load build context\n",
      "#15 transferring context: 7.50kB done\n",
      "#15 DONE 0.0s\n",
      "\n",
      "#16 [worker 3/8] RUN apt-get update && apt-get install -y --no-install-recommends     gcc libpq-dev curl unzip     && rm -rf /var/lib/apt/lists/*\n",
      "#16 CACHED\n",
      "\n",
      "#17 [worker 6/8] COPY pyproject.toml .\n",
      "#17 CACHED\n",
      "\n",
      "#18 [worker 5/8] RUN pip install --no-cache-dir uv\n",
      "#18 CACHED\n",
      "\n",
      "#19 [worker 2/8] WORKDIR /app\n",
      "#19 CACHED\n",
      "\n",
      "#20 [worker 7/8] COPY app/ app/\n",
      "#20 CACHED\n",
      "\n",
      "#21 [worker 4/8] RUN curl -fsSL https://rclone.org/install.sh | bash\n",
      "#21 CACHED\n",
      "\n",
      "#22 [worker 8/8] RUN uv pip install --system --no-cache .\n",
      "#22 CACHED\n",
      "\n",
      "#23 [api] exporting to image\n",
      "#23 exporting layers done\n",
      "#23 exporting manifest sha256:91d3348a004a409ea0fef172b1c4d7ee2c963f6930fd9f2f3779939a31eac976 done\n",
      "#23 exporting config sha256:a8894e9fe17bf508badb1b836dbeecef25008dbb57818fef392d3ba20e18f3f3 done\n",
      "#23 exporting attestation manifest sha256:07e38fd4f9f8082ea7a31de3019b0cf3ccf73dfa477df13fd705925249638d7c 0.0s done\n",
      "#23 exporting manifest list sha256:64c44979c438fdd31082753a7e27f87cc794fea7d8bfc5da73b10bd4b020bfb7 done\n",
      "#23 naming to docker.io/library/streamweave-api:latest done\n",
      "#23 unpacking to docker.io/library/streamweave-api:latest done\n",
      "#23 DONE 0.1s\n",
      "\n",
      "#24 [worker] exporting to image\n",
      "#24 exporting layers done\n",
      "#24 exporting manifest sha256:d2e8b9b5ab3ff703d56a4391e6f449df75b221e72a6b77935f30e7d61b3c5689 done\n",
      "#24 exporting config sha256:92b5afa2c36f6b338096679c75b0a90389754b58be5836c10f17ba8ac7696c31 done\n",
      "#24 exporting attestation manifest sha256:dd9c1148d832db19428c66d854413a0f647042c1c53c6463492440e6ec15f8b5 0.0s done\n",
      "#24 exporting manifest list sha256:52ea0523494fa96067a26fb34b3e79102c0297a7dccea8d9d773bd661db832c2 done\n",
      "#24 naming to docker.io/library/streamweave-worker:latest done\n",
      "#24 unpacking to docker.io/library/streamweave-worker:latest done\n",
      "#24 DONE 0.1s\n",
      "\n",
      "#25 [worker] resolving provenance for metadata file\n",
      "#25 DONE 0.0s\n",
      "\n",
      "#26 [api] resolving provenance for metadata file\n",
      "#26 DONE 0.0s\n",
      "\u001b[33m Image streamweave-worker Built \n",
      "\u001b[0m\u001b[33m Image streamweave-api Built \n",
      "\u001b[0m\u001b[33m Network streamweave_default Creating \n",
      "\u001b[0m\u001b[33m Network streamweave_default Created \n",
      "\u001b[0m\u001b[33m Volume streamweave_prefect_pgdata Creating \n",
      "\u001b[0m\u001b[33m Volume streamweave_prefect_pgdata Created \n",
      "\u001b[0m\u001b[33m Volume streamweave_harvest_data Creating \n",
      "\u001b[0m\u001b[33m Volume streamweave_harvest_data Created \n",
      "\u001b[0m\u001b[33m Volume streamweave_pgdata Creating \n",
      "\u001b[0m\u001b[33m Volume streamweave_pgdata Created \n",
      "\u001b[0m\u001b[33m Container streamweave-redis-1 Creating \n",
      "\u001b[0m\u001b[33m Container streamweave-postgres-1 Creating \n",
      "\u001b[0m\u001b[33m Container streamweave-prefect-postgres-1 Creating \n",
      "\u001b[0m\u001b[33m Container streamweave-redis-1 Created \n",
      "\u001b[0m\u001b[33m Container streamweave-prefect-postgres-1 Created \n",
      "\u001b[0m\u001b[33m Container streamweave-prefect-server-1 Creating \n",
      "\u001b[0m\u001b[33m Container streamweave-postgres-1 Created \n",
      "\u001b[0m\u001b[33m Container streamweave-prefect-server-1 Created \n",
      "\u001b[0m\u001b[33m Container streamweave-api-1 Creating \n",
      "\u001b[0m\u001b[33m Container streamweave-worker-1 Creating \n",
      "\u001b[0m\u001b[33m Container streamweave-worker-1 Created \n",
      "\u001b[0m\u001b[33m Container streamweave-api-1 Created \n",
      "\u001b[0m\u001b[33m Container streamweave-prefect-postgres-1 Starting \n",
      "\u001b[0m\u001b[33m Container streamweave-redis-1 Starting \n",
      "\u001b[0m\u001b[33m Container streamweave-postgres-1 Starting \n",
      "\u001b[0m\u001b[33m Container streamweave-prefect-postgres-1 Started \n",
      "\u001b[0m\u001b[33m Container streamweave-redis-1 Started \n",
      "\u001b[0m\u001b[33m Container streamweave-prefect-postgres-1 Waiting \n",
      "\u001b[0m\u001b[33m Container streamweave-redis-1 Waiting \n",
      "\u001b[0m\u001b[33m Container streamweave-postgres-1 Started \n",
      "\u001b[0m\u001b[33m Container streamweave-redis-1 Healthy \n",
      "\u001b[0m\u001b[33m Container streamweave-prefect-postgres-1 Healthy \n",
      "\u001b[0m\u001b[33m Container streamweave-prefect-server-1 Starting \n",
      "\u001b[0m\u001b[33m Container streamweave-prefect-server-1 Started \n",
      "\u001b[0m\u001b[33m Container streamweave-postgres-1 Waiting \n",
      "\u001b[0m\u001b[33m Container streamweave-postgres-1 Waiting \n",
      "\u001b[0m\u001b[33m Container streamweave-postgres-1 Healthy \n",
      "\u001b[0m\u001b[33m Container streamweave-worker-1 Starting \n",
      "\u001b[0m\u001b[33m Container streamweave-postgres-1 Healthy \n",
      "\u001b[0m\u001b[33m Container streamweave-api-1 Starting \n",
      "\u001b[0m\u001b[33m Container streamweave-api-1 Started \n",
      "\u001b[0m\u001b[33m Container streamweave-worker-1 Started \n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "_ = run(\"docker compose down -v\")\n",
    "_ = run(\"docker compose up -d --build\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This starts:\n",
    "- **postgres** \u2014 app database (port 5432)\n",
    "- **redis** \u2014 Prefect cache (port 6379)\n",
    "- **prefect-postgres** \u2014 Prefect's own database\n",
    "- **prefect-server** \u2014 Prefect UI + API (port 4200)\n",
    "- **api** \u2014 Streamweave FastAPI (port 8000)\n",
    "- **worker** \u2014 Prefect worker with rclone installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                             IMAGE                        COMMAND                  SERVICE            CREATED         STATUS                   PORTS\n",
      "streamweave-api-1                streamweave-api              \"sh -c 'alembic upgr\u2026\"   api                7 seconds ago   Up Less than a second    0.0.0.0:8000->8000/tcp, [::]:8000->8000/tcp\n",
      "streamweave-postgres-1           postgres:16-alpine           \"docker-entrypoint.s\u2026\"   postgres           7 seconds ago   Up 6 seconds (healthy)   0.0.0.0:5432->5432/tcp, [::]:5432->5432/tcp\n",
      "streamweave-prefect-postgres-1   postgres:16-alpine           \"docker-entrypoint.s\u2026\"   prefect-postgres   7 seconds ago   Up 6 seconds (healthy)   5432/tcp\n",
      "streamweave-prefect-server-1     prefecthq/prefect:3-latest   \"/usr/bin/tini -g --\u2026\"   prefect-server     7 seconds ago   Up Less than a second    0.0.0.0:4200->4200/tcp, [::]:4200->4200/tcp\n",
      "streamweave-redis-1              redis:7-alpine               \"docker-entrypoint.s\u2026\"   redis              7 seconds ago   Up 6 seconds (healthy)   0.0.0.0:6379->6379/tcp, [::]:6379->6379/tcp\n",
      "streamweave-worker-1             streamweave-worker           \"prefect worker star\u2026\"   worker             7 seconds ago   Up Less than a second    \n"
     ]
    }
   ],
   "source": [
    "_ = run(\"docker compose ps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check api status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for API... (attempt 1/30)\n",
      "Waiting for API... (attempt 2/30)\n",
      "Waiting for API... (attempt 3/30)\n",
      "{\n",
      "  \"status\": \"ok\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Wait for the API to be ready (retries up to 30 seconds)\n",
    "for attempt in range(30):\n",
    "    try:\n",
    "        resp = client.get(\"/health\")\n",
    "        pp(resp)\n",
    "        break\n",
    "    except httpx.RequestError:\n",
    "        print(f\"Waiting for API... (attempt {attempt + 1}/30)\")\n",
    "        time.sleep(1)\n",
    "else:\n",
    "    raise RuntimeError(\"API did not become available within 30 seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected: `{\"status\": \"ok\"}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prefect UI is at http://localhost:4200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Admin User\n",
    "\n",
    "The `create-admin.py` script is interactive, but also accepts arguments to set the admin user's username and password:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Admin user created: admin@test.org (id=599dac7f-0685-4ab2-951d-c354bdcb4d00)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "ADMIN_EMAIL = \"admin@test.org\"\n",
    "ADMIN_PASSWORD = \"adminpassword123\"\n",
    "\n",
    "_ = run(\n",
    "    f\"python {REPO_ROOT / 'scripts' / 'create-admin.py'}\"\n",
    "    f\" --email {ADMIN_EMAIL} --password {ADMIN_PASSWORD}\",\n",
    "    env={**os.environ, \"DATABASE_URL\": \"postgresql+asyncpg://streamweave:streamweave@localhost:5432/streamweave\"},\n",
    "    cwd=str(REPO_ROOT / \"backend\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Get an Auth Token\n",
    "\n",
    "Replace the email/password below with what you used in the admin creation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token acquired (first 20 chars): eyJhbGciOiJIUzI1NiIs...\n"
     ]
    }
   ],
   "source": [
    "resp = client.post(\"/auth/jwt/login\", data={\"username\": ADMIN_EMAIL, \"password\": ADMIN_PASSWORD})\n",
    "TOKEN = resp.json()[\"access_token\"]\n",
    "AUTH = {\"Authorization\": f\"Bearer {TOKEN}\"}\n",
    "print(f\"Token acquired (first 20 chars): {TOKEN[:20]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Seed the Database\n",
    "\n",
    "Run the seed script locally to create instruments, storage locations, schedules, and hooks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simlab seed data created successfully.\n",
      "  Service account: simlab-service (id=57f526ca-efc3-45c2-92b7-34ff999ff122)\n",
      "  Instruments: Microscope 01, Spectrometer 01, X-Ray Diffraction 01\n",
      "  Storage: Archive Storage, Restricted Storage\n",
      "  Schedules: 3 created\n",
      "  Hooks: 2 created\n"
     ]
    }
   ],
   "source": [
    "_ = run(\n",
    "    \"python seed.py\",\n",
    "    cwd=str(REPO_ROOT / \"simlab\"),\n",
    ")\n",
    "\n",
    "# sleep to allow seeding to finish when running notebook all at once\n",
    "time.sleep(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Verify Seeded Data\n",
    "\n",
    "### 5a. List instruments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"id\": \"4bca34e9-5309-4d8d-b78c-af9ef64845f8\",\n",
      "    \"name\": \"Microscope 01\",\n",
      "    \"description\": \"Simulated optical microscope\",\n",
      "    \"location\": \"Lab A, Room 101\",\n",
      "    \"pid\": null,\n",
      "    \"cifs_host\": \"microscope-01\",\n",
      "    \"cifs_share\": \"microscope\",\n",
      "    \"cifs_base_path\": \"/\",\n",
      "    \"service_account_id\": \"57f526ca-efc3-45c2-92b7-34ff999ff122\",\n",
      "    \"transfer_adapter\": \"rclone\",\n",
      "    \"transfer_config\": null,\n",
      "    \"enabled\": true,\n",
      "    \"created_at\": \"2026-02-24T17:05:37.343035Z\",\n",
      "    \"updated_at\": \"2026-02-24T17:05:37.343035Z\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"65186f54-074d-430e-b2ca-c24b4861e990\",\n",
      "    \"name\": \"Spectrometer 01\",\n",
      "    \"description\": \"Simulated UV-Vis spectrometer\",\n",
      "    \"location\": \"Lab B, Room 205\",\n",
      "    \"pid\": null,\n",
      "    \"cifs_host\": \"spectrometer-01\",\n",
      "    \"cifs_share\": \"spectrometer\",\n",
      "    \"cifs_base_path\": \"/\",\n",
      "    \"service_account_id\": \"57f526ca-efc3-45c2-92b7-34ff999ff122\",\n",
      "    \"transfer_adapter\": \"rclone\",\n",
      "    \"transfer_config\": null,\n",
      "    \"enabled\": true,\n",
      "    \"created_at\": \"2026-02-24T17:05:37.343035Z\",\n",
      "    \"updated_at\": \"2026-02-24T17:05:37.343035Z\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"20bfa6b2-87a4-4d41-9c97-d7f59bd92723\",\n",
      "    \"name\": \"X-Ray Diffraction 01\",\n",
      "    \"description\": \"Simulated X-ray diffractometer\",\n",
      "    \"location\": \"Lab C, Room 310\",\n",
      "    \"pid\": null,\n",
      "    \"cifs_host\": \"xray-diffraction-01\",\n",
      "    \"cifs_share\": \"xrd\",\n",
      "    \"cifs_base_path\": \"/\",\n",
      "    \"service_account_id\": \"57f526ca-efc3-45c2-92b7-34ff999ff122\",\n",
      "    \"transfer_adapter\": \"rclone\",\n",
      "    \"transfer_config\": null,\n",
      "    \"enabled\": true,\n",
      "    \"created_at\": \"2026-02-24T17:05:37.343035Z\",\n",
      "    \"updated_at\": \"2026-02-24T17:05:37.343035Z\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "resp = client.get(\"/api/instruments\", headers=AUTH)\n",
    "pp(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected: 3 instruments (Microscope 01, Spectrometer 01, X-Ray Diffraction 01)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b. List storage locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"id\": \"7cb0e261-f511-4ff3-96af-0bd3323d0ba7\",\n",
      "    \"name\": \"Archive Storage\",\n",
      "    \"type\": \"posix\",\n",
      "    \"connection_config\": {},\n",
      "    \"base_path\": \"/storage/archive\",\n",
      "    \"enabled\": true,\n",
      "    \"created_at\": \"2026-02-24T17:05:37.343035Z\",\n",
      "    \"updated_at\": \"2026-02-24T17:05:37.343035Z\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"51c43e6b-e524-4fd0-aa92-3b40d8fe7cda\",\n",
      "    \"name\": \"Restricted Storage\",\n",
      "    \"type\": \"posix\",\n",
      "    \"connection_config\": {},\n",
      "    \"base_path\": \"/storage/restricted\",\n",
      "    \"enabled\": true,\n",
      "    \"created_at\": \"2026-02-24T17:05:37.343035Z\",\n",
      "    \"updated_at\": \"2026-02-24T17:05:37.343035Z\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "resp = client.get(\"/api/storage-locations\", headers=AUTH)\n",
    "pp(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected: 2 locations (Archive Storage at `/storage/archive`, Restricted Storage at `/storage/restricted`).\n",
    "\n",
    "### 5c. List schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"id\": \"29f057d4-4d8d-4de9-b741-390c4b5ac0c2\",\n",
      "    \"instrument_id\": \"4bca34e9-5309-4d8d-b78c-af9ef64845f8\",\n",
      "    \"default_storage_location_id\": \"7cb0e261-f511-4ff3-96af-0bd3323d0ba7\",\n",
      "    \"cron_expression\": \"*/15 * * * *\",\n",
      "    \"prefect_deployment_id\": null,\n",
      "    \"enabled\": true,\n",
      "    \"created_at\": \"2026-02-24T17:05:37.343035Z\",\n",
      "    \"updated_at\": \"2026-02-24T17:05:37.343035Z\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"1649bd4b-b44d-4ca0-a699-533dcc75adee\",\n",
      "    \"instrument_id\": \"65186f54-074d-430e-b2ca-c24b4861e990\",\n",
      "    \"default_storage_location_id\": \"7cb0e261-f511-4ff3-96af-0bd3323d0ba7\",\n",
      "    \"cron_expression\": \"*/15 * * * *\",\n",
      "    \"prefect_deployment_id\": null,\n",
      "    \"enabled\": true,\n",
      "    \"created_at\": \"2026-02-24T17:05:37.343035Z\",\n",
      "    \"updated_at\": \"2026-02-24T17:05:37.343035Z\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"43b115e4-7fef-42cd-b781-dc70a7381a40\",\n",
      "    \"instrument_id\": \"20bfa6b2-87a4-4d41-9c97-d7f59bd92723\",\n",
      "    \"default_storage_location_id\": \"51c43e6b-e524-4fd0-aa92-3b40d8fe7cda\",\n",
      "    \"cron_expression\": \"*/15 * * * *\",\n",
      "    \"prefect_deployment_id\": null,\n",
      "    \"enabled\": true,\n",
      "    \"created_at\": \"2026-02-24T17:05:37.343035Z\",\n",
      "    \"updated_at\": \"2026-02-24T17:05:37.343035Z\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "resp = client.get(\"/api/schedules\", headers=AUTH)\n",
    "schedules = resp.json()\n",
    "pp(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected: 3 schedules with `cron_expression: \"*/15 * * * *\"`. Note that `prefect_deployment_id` will be **null** \u2014 the seed script writes directly to the DB and doesn't create Prefect deployments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5c. Re-create schedules through the API\n",
    "\n",
    "The seed script (for demonstration purposes) bypasses the API, so schedules have no Prefect deployments. Delete the seeded schedules and re-create them through the API, which triggers deployment creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 3 seeded schedules\n",
      "{\n",
      "  \"id\": \"725fa608-f4da-40f5-980a-2aa977ffe7e3\",\n",
      "  \"prefect_deployment_id\": \"a28a83b2-4d95-45ba-acd3-6b664c9ba1e7\"\n",
      "}\n",
      "{\n",
      "  \"id\": \"0293924e-a6de-459c-98c0-6d73303d576b\",\n",
      "  \"prefect_deployment_id\": \"97ee019b-aa52-43b0-84e9-f472b9951c50\"\n",
      "}\n",
      "{\n",
      "  \"id\": \"a86e9e55-9518-4b9f-9a24-321f3ea3b428\",\n",
      "  \"prefect_deployment_id\": \"decb9a53-cd72-4922-959c-9a955338e70f\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Get instrument and storage IDs\n",
    "instruments = client.get(\"/api/instruments\", headers=AUTH).json()\n",
    "storage_locs = client.get(\"/api/storage-locations\", headers=AUTH).json()\n",
    "STORAGE_ID = storage_locs[0][\"id\"]\n",
    "RESTRICTED_ID = storage_locs[1][\"id\"]\n",
    "\n",
    "# Delete existing seeded schedules\n",
    "for s in schedules:\n",
    "    client.delete(f\"/api/schedules/{s['id']}\", headers=AUTH)\n",
    "print(f\"Deleted {len(schedules)} seeded schedules\")\n",
    "\n",
    "# Re-create via API (this creates Prefect deployments)\n",
    "for i, inst in enumerate(instruments):\n",
    "    stor = RESTRICTED_ID if i == 2 else STORAGE_ID\n",
    "    resp = client.post(\"/api/schedules\", headers=AUTH, json={\n",
    "        \"instrument_id\": inst[\"id\"],\n",
    "        \"default_storage_location_id\": stor,\n",
    "        \"cron_expression\": \"*/15 * * * *\",\n",
    "        \"enabled\": True,\n",
    "    })\n",
    "    data = resp.json()\n",
    "    print(json.dumps({\"id\": data[\"id\"], \"prefect_deployment_id\": data.get(\"prefect_deployment_id\")}, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each schedule should now an `id` and also show a non-null `prefect_deployment_id`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the first schedule ID for later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schedule ID: 725fa608-f4da-40f5-980a-2aa977ffe7e3\n"
     ]
    }
   ],
   "source": [
    "SCHEDULE_ID = client.get(\"/api/schedules\", headers=AUTH).json()[0][\"id\"]\n",
    "print(f\"Schedule ID: {SCHEDULE_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5d. List hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"id\": \"b77c8db2-8fdf-4995-9739-cc03289297bd\",\n",
      "    \"name\": \"Microscope File Filter\",\n",
      "    \"description\": \"Exclude temporary files from microscope harvests\",\n",
      "    \"trigger\": \"pre_transfer\",\n",
      "    \"implementation\": \"builtin\",\n",
      "    \"builtin_name\": \"file_filter\",\n",
      "    \"script_path\": null,\n",
      "    \"webhook_url\": null,\n",
      "    \"config\": {\n",
      "      \"exclude_patterns\": [\n",
      "        \"*.tmp\",\n",
      "        \"*.lock\",\n",
      "        \"~$*\"\n",
      "      ]\n",
      "    },\n",
      "    \"instrument_id\": \"4bca34e9-5309-4d8d-b78c-af9ef64845f8\",\n",
      "    \"priority\": 0,\n",
      "    \"enabled\": true\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"0d6f6b94-b1ed-466d-8e8e-a26cbdfc43dd\",\n",
      "    \"name\": \"Microscope Metadata Enrichment\",\n",
      "    \"description\": \"Extract experiment and run info from file paths\",\n",
      "    \"trigger\": \"post_transfer\",\n",
      "    \"implementation\": \"builtin\",\n",
      "    \"builtin_name\": \"metadata_enrichment\",\n",
      "    \"script_path\": null,\n",
      "    \"webhook_url\": null,\n",
      "    \"config\": {\n",
      "      \"rules\": [\n",
      "        {\n",
      "          \"pattern\": \"/(?P<username>[a-z][a-z0-9_]*)/(?P<experiment>experiment_\\\\d+)\",\n",
      "          \"source\": \"path\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    \"instrument_id\": \"4bca34e9-5309-4d8d-b78c-af9ef64845f8\",\n",
      "    \"priority\": 0,\n",
      "    \"enabled\": true\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "resp = client.get(\"/api/hooks\", headers=AUTH)\n",
    "pp(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected: 2 hooks on the microscope:\n",
    "\n",
    "- **Microscope File Filter** (pre_transfer): excludes `*.tmp`, `*.lock`, `~$*`\n",
    "- **Microscope Metadata Enrichment** (post_transfer): extracts `username` and `experiment` from paths like `{username}/{experiment}/...`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Prefect Integration\n",
    "\n",
    "### 6a. Check Prefect UI\n",
    "\n",
    "Open http://localhost:4200 in a browser. After re-creating schedules via the API (step 5c), you should see:\n",
    "\n",
    "**Deployments** tab: 3 deployments named `harvest-{instrument_name}`:\n",
    "\n",
    "<img style=\"margin-left: 4em;\" src=\"_static/20260224_prefect_deployments.png\" width=\"800\">\n",
    "\n",
    "**Work Pools** tab: a pool named **streamweave-worker-pool** with an active worker:\n",
    "\n",
    "<img style=\"margin-left: 4em;\" src=\"_static/20260224_prefect_workpools.png\" width=\"800\">\n",
    "\n",
    "The **harvest-instrument** flow will appear under Flows once the first run is triggered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6b. Trigger a manual harvest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"flow_run_id\": \"ebf11ded-7ff8-4388-be9d-53324abaab5a\",\n",
      "  \"schedule_id\": \"725fa608-f4da-40f5-980a-2aa977ffe7e3\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "resp = client.post(f\"/api/schedules/{SCHEDULE_ID}/trigger\", headers=AUTH)\n",
    "pp(resp)\n",
    "FLOW_RUN_ID = resp.json().get(\"flow_run_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected response:\n",
    "```json\n",
    "{\n",
    "  \"flow_run_id\": \"<uuid>\",\n",
    "  \"schedule_id\": \"<uuid>\"\n",
    "}\n",
    "```\n",
    "\n",
    "If you get a `400` error about \"no Prefect deployment\", the schedule wasn't linked to Prefect during seeding. Re-create the schedule through the API (see step 5c-extra)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6c. Monitor in Prefect UI\n",
    "\n",
    "Go to http://localhost:4200/flow-runs and watch the triggered flow run. It will:\n",
    "\n",
    "1. Run `discover_files_task` \u2014 discovers files from the instrument's Samba share (2 in this example)\n",
    "2. Run `transfer_single_file_task` for each new file \u2014 transfers via rclone (2)\n",
    "\n",
    "#### *Flow run diagram*\n",
    "\n",
    "This diagram shows the timing and status (green = success; red = error) of the different parts of the flow. In this figure, we can see one `discover_files_task` and six `transfer_single_file_task` runs for each file that needed to be transferred:\n",
    "\n",
    "<img style=\"margin-left: 4em;\" src=\"_static/20260224_prefect_flowrun_figure.png\" width=\"800\">\n",
    "\n",
    "#### *Logs*\n",
    "\n",
    "The full logs for the flow run are shown in the Prefect UI, which is very useful for debugging:\n",
    "\n",
    "<img style=\"margin-left: 4em;\" src=\"_static/20260224_prefect_flowrun_logs.png\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Verify Harvest Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7a. Check file records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for flow run ebf11ded-7ff8-4388-be9d-53324abaab5a to complete...\n",
      "  State: SCHEDULED (attempt 1/120)\n",
      "  State: SCHEDULED (attempt 2/120)\n",
      "  State: SCHEDULED (attempt 3/120)\n",
      "  State: SCHEDULED (attempt 4/120)\n",
      "  State: SCHEDULED (attempt 5/120)\n",
      "  State: PENDING (attempt 6/120)\n",
      "  State: PENDING (attempt 7/120)\n",
      "Flow run finished with state: COMPLETED\n",
      "\n",
      "Found 2 files\n",
      "[\n",
      "  {\n",
      "    \"id\": \"f6b5344e-2f79-43c9-b8d4-9d43d166f027\",\n",
      "    \"persistent_id\": \"ark:/99999/fk4hwtjkzdjnzaa7bqundhd35qkzm\",\n",
      "    \"persistent_id_type\": \"ark\",\n",
      "    \"instrument_id\": \"4bca34e9-5309-4d8d-b78c-af9ef64845f8\",\n",
      "    \"source_path\": \"microscope/experiment_002.csv\",\n",
      "    \"filename\": \"experiment_002.csv\",\n",
      "    \"size_bytes\": 174,\n",
      "    \"source_mtime\": \"2026-02-23T15:59:36.848000Z\",\n",
      "    \"xxhash\": \"4a76430697fcf84b\",\n",
      "    \"sha256\": null,\n",
      "    \"first_discovered_at\": \"2026-02-24T17:05:49.906603Z\",\n",
      "    \"metadata_\": {},\n",
      "    \"owner_id\": null\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"6b64c77c-100a-4e99-8a8c-8ec6cce076e1\",\n",
      "    \"persistent_id\": \"ark:/99999/fk455jyy3qsf5fsnm2yrmay4tw2pu\",\n",
      "    \"persistent_id_type\": \"ark\",\n",
      "    \"instrument_id\": \"4bca34e9-5309-4d8d-b78c-af9ef64845f8\",\n",
      "    \"source_path\": \"microscope/experiment_001.csv\",\n",
      "    \"filename\": \"experiment_001.csv\",\n",
      "    \"size_bytes\": 302,\n",
      "    \"source_mtime\": \"2026-02-23T15:59:35.311000Z\",\n",
      "    \"xxhash\": \"2660dec29d8c3f7b\",\n",
      "    \"sha256\": null,\n",
      "    \"first_discovered_at\": \"2026-02-24T17:05:49.838927Z\",\n",
      "    \"metadata_\": {},\n",
      "    \"owner_id\": null\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "wait_for_flow_run(FLOW_RUN_ID)\n",
    "\n",
    "resp = client.get(\"/api/files\", headers=AUTH)\n",
    "print(f\"\\nFound {len(resp.json())} files\")\n",
    "pp(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each file you should see:\n",
    "- `persistent_id` starting with `ark:/99999/fk4...` (unique ARK identifier)\n",
    "- `instrument_id` matching the harvested instrument\n",
    "- `source_path` matching the file's path on the instrument\n",
    "- `filename` \u2014 the file name\n",
    "- `xxhash` \u2014 checksum computed after transfer\n",
    "\n",
    "### 7b. Check transfer records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for flow run ebf11ded-7ff8-4388-be9d-53324abaab5a to complete...\n",
      "Flow run finished with state: COMPLETED\n",
      "Found 2 transfers\n",
      "[\n",
      "  {\n",
      "    \"id\": \"3ff499b1-7989-4bee-b539-58d6eea94e9d\",\n",
      "    \"file_id\": \"6b64c77c-100a-4e99-8a8c-8ec6cce076e1\",\n",
      "    \"storage_location_id\": \"7cb0e261-f511-4ff3-96af-0bd3323d0ba7\",\n",
      "    \"destination_path\": \"/storage/archive/Microscope 01/microscope/experiment_001.csv\",\n",
      "    \"transfer_adapter\": \"rclone\",\n",
      "    \"status\": \"completed\",\n",
      "    \"bytes_transferred\": 302,\n",
      "    \"source_checksum\": null,\n",
      "    \"dest_checksum\": \"2660dec29d8c3f7b\",\n",
      "    \"checksum_verified\": false,\n",
      "    \"started_at\": \"2026-02-24T17:05:49.842858Z\",\n",
      "    \"completed_at\": \"2026-02-24T17:05:49.896039Z\",\n",
      "    \"error_message\": null,\n",
      "    \"prefect_flow_run_id\": null\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"1164b5a5-40f7-44aa-ba06-6009fc9ef0b2\",\n",
      "    \"file_id\": \"f6b5344e-2f79-43c9-b8d4-9d43d166f027\",\n",
      "    \"storage_location_id\": \"7cb0e261-f511-4ff3-96af-0bd3323d0ba7\",\n",
      "    \"destination_path\": \"/storage/archive/Microscope 01/microscope/experiment_002.csv\",\n",
      "    \"transfer_adapter\": \"rclone\",\n",
      "    \"status\": \"completed\",\n",
      "    \"bytes_transferred\": 174,\n",
      "    \"source_checksum\": null,\n",
      "    \"dest_checksum\": \"4a76430697fcf84b\",\n",
      "    \"checksum_verified\": false,\n",
      "    \"started_at\": \"2026-02-24T17:05:49.907783Z\",\n",
      "    \"completed_at\": \"2026-02-24T17:05:49.956082Z\",\n",
      "    \"error_message\": null,\n",
      "    \"prefect_flow_run_id\": null\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "wait_for_flow_run(FLOW_RUN_ID)\n",
    "resp = client.get(\"/api/transfers\", headers=AUTH)\n",
    "print(f\"Found {len(resp.json())} transfers\")\n",
    "pp(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each transfer should have:\n",
    "- `status`: `\"completed\"` or `\"skipped\"`\n",
    "- `dest_checksum` \u2014 xxhash of the transferred file\n",
    "- `destination_path` \u2014 where the file was written under `/storage/`\n",
    "- `bytes_transferred` \u2014 file size\n",
    "- `started_at` and `completed_at` timestamps\n",
    "\n",
    "### 7c. Verify files on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/archive/\n",
      "\u2514\u2500\u2500 Microscope 01\n",
      "    \u2514\u2500\u2500 microscope\n",
      "        \u251c\u2500\u2500 experiment_001.csv\n",
      "        \u2514\u2500\u2500 experiment_002.csv\n",
      "\n",
      "3 directories, 2 files\n"
     ]
    }
   ],
   "source": [
    "_ = run(\"docker compose exec api tree /storage/archive/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8a. Testing ignoring files via pre-transfer hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16K    \n",
      "drwxr-xr-x    5 root     root         160 Feb 24 17:05 .\n",
      "drwxr-xr-x    1 root     root        4.0K Feb 24 17:05 ..\n",
      "-rw-r--r--    1 root     root         302 Feb 23 15:59 experiment_001.csv\n",
      "-rw-r--r--    1 root     root         174 Feb 23 15:59 experiment_002.csv\n",
      "-rw-r--r--    1 root     root          10 Feb 24 17:05 scratch.tmp\n"
     ]
    }
   ],
   "source": [
    "# create a file for the microscope-01 instrument that matches the exclusion filter in the built-in pre-transfer hook\n",
    "_ = run(f\"docker compose -f {SIMLAB_COMPOSE} exec microscope-01 sh -c 'echo temp data > /data/scratch.tmp'\")\n",
    "_ = run(f\"docker compose -f {SIMLAB_COMPOSE} exec microscope-01 ls -lah /data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8b. Trigger another harvest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"flow_run_id\": \"531c0deb-9f72-486d-a370-374eecd3dc4e\",\n",
      "  \"schedule_id\": \"725fa608-f4da-40f5-980a-2aa977ffe7e3\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "resp = client.post(f\"/api/schedules/{SCHEDULE_ID}/trigger\", headers=AUTH)\n",
    "IGNORE_FLOW_RUN_ID = resp.json().get(\"flow_run_id\")\n",
    "pp(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8c. Verify the .tmp file was skipped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StreamWeave has a demonstration pre-transfer hook that ignores certain file patterns. These can be configured easily on a per-instrument basis. The following example will show that the `scratch.tmp` file is discovered in the file finding flow, but is not transferred due to the pre-transfer hook blocking it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for flow run 531c0deb-9f72-486d-a370-374eecd3dc4e to complete...\n",
      "  State: SCHEDULED (attempt 1/120)\n",
      "  State: SCHEDULED (attempt 2/120)\n",
      "  State: SCHEDULED (attempt 3/120)\n",
      "  State: SCHEDULED (attempt 4/120)\n",
      "  State: SCHEDULED (attempt 5/120)\n",
      "  State: SCHEDULED (attempt 6/120)\n",
      "  State: SCHEDULED (attempt 7/120)\n",
      "  State: PENDING (attempt 8/120)\n",
      "Flow run finished with state: COMPLETED\n",
      "Files:\n",
      "------\n",
      "[\n",
      "  {\n",
      "    \"id\": \"0fa9c05f-73e4-4939-bf45-b724e43f07ac\",\n",
      "    \"persistent_id\": \"ark:/99999/fk4254liryuqvggvc4dmaqhtrczpu\",\n",
      "    \"persistent_id_type\": \"ark\",\n",
      "    \"instrument_id\": \"4bca34e9-5309-4d8d-b78c-af9ef64845f8\",\n",
      "    \"source_path\": \"microscope/scratch.tmp\",\n",
      "    \"filename\": \"scratch.tmp\",\n",
      "    \"size_bytes\": 10,\n",
      "    \"source_mtime\": \"2026-02-24T17:05:50.533000Z\",\n",
      "    \"xxhash\": null,\n",
      "    \"sha256\": null,\n",
      "    \"first_discovered_at\": \"2026-02-24T17:05:58.485254Z\",\n",
      "    \"metadata_\": {},\n",
      "    \"owner_id\": null\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"f6b5344e-2f79-43c9-b8d4-9d43d166f027\",\n",
      "    \"persistent_id\": \"ark:/99999/fk4hwtjkzdjnzaa7bqundhd35qkzm\",\n",
      "    \"persistent_id_type\": \"ark\",\n",
      "    \"instrument_id\": \"4bca34e9-5309-4d8d-b78c-af9ef64845f8\",\n",
      "    \"source_path\": \"microscope/experiment_002.csv\",\n",
      "    \"filename\": \"experiment_002.csv\",\n",
      "    \"size_bytes\": 174,\n",
      "    \"source_mtime\": \"2026-02-23T15:59:36.848000Z\",\n",
      "    \"xxhash\": \"4a76430697fcf84b\",\n",
      "    \"sha256\": null,\n",
      "    \"first_discovered_at\": \"2026-02-24T17:05:49.906603Z\",\n",
      "    \"metadata_\": {},\n",
      "    \"owner_id\": null\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"6b64c77c-100a-4e99-8a8c-8ec6cce076e1\",\n",
      "    \"persistent_id\": \"ark:/99999/fk455jyy3qsf5fsnm2yrmay4tw2pu\",\n",
      "    \"persistent_id_type\": \"ark\",\n",
      "    \"instrument_id\": \"4bca34e9-5309-4d8d-b78c-af9ef64845f8\",\n",
      "    \"source_path\": \"microscope/experiment_001.csv\",\n",
      "    \"filename\": \"experiment_001.csv\",\n",
      "    \"size_bytes\": 302,\n",
      "    \"source_mtime\": \"2026-02-23T15:59:35.311000Z\",\n",
      "    \"xxhash\": \"2660dec29d8c3f7b\",\n",
      "    \"sha256\": null,\n",
      "    \"first_discovered_at\": \"2026-02-24T17:05:49.838927Z\",\n",
      "    \"metadata_\": {},\n",
      "    \"owner_id\": null\n",
      "  }\n",
      "]\n",
      "PASS: scratch.tmp was correctly filtered by the pre-transfer hook (transfer skipped)\n"
     ]
    }
   ],
   "source": [
    "wait_for_flow_run(IGNORE_FLOW_RUN_ID)\n",
    "\n",
    "print(\"Files:\\n------\")\n",
    "resp = client.get(\"/api/files\", headers=AUTH)\n",
    "pp(resp)\n",
    "# microscope/scratch.tmp will be in the file list printed out at this step\n",
    "\n",
    "files = resp.json()\n",
    "scratch = next((f for f in files if f[\"filename\"] == \"scratch.tmp\"), None)\n",
    "# firmly assert that the file was found\n",
    "assert scratch is not None, \"FAIL: scratch.tmp file record not found\"\n",
    "\n",
    "transfers = client.get(f\"/api/transfers?file_id={scratch['id']}\", headers=AUTH).json()\n",
    "# firmly assert that the file was not transferred\n",
    "assert all(t[\"status\"] == \"skipped\" for t in transfers), \"FAIL: scratch.tmp should only have skipped transfers\"\n",
    "print(\"PASS: scratch.tmp was correctly filtered by the pre-transfer hook (transfer skipped)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prefect logs show the file being skipped:\n",
    "\n",
    "<img style=\"margin-left: 4em;\" src=\"_static/20260224_prefect_flowrun_logs_skip_tmp.png\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test Post-Transfer Hook (Metadata Enrichment)\n",
    "\n",
    "The microscope harvester also has a demonstration post-transfer hook that extracts `username` and `experiment` from file paths matching the pattern `{username}/{experiment}/...`. This demonstrates how you can generate simple metadata extraction pipelines to operate as part of the data transfer process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add three example CSV files in different folders for two different users:\n",
    "_ = run(f\"\"\"docker compose -f {SIMLAB_COMPOSE} exec microscope-01 sh -c '\n",
    "mkdir -p /data/user_a/experiment_001 /data/user_a/experiment_002 /data/user_b/experiment_003 &&\n",
    "printf \"time,value\\\\n0,1.0\\\\n1,2.0\\\\n\" > /data/user_a/experiment_001/scan_01.csv &&\n",
    "printf \"time,value\\\\n0,1.0\\\\n1,2.0\\\\n\" > /data/user_a/experiment_002/scan_01.csv &&\n",
    "printf \"time,value\\\\n0,1.0\\\\n1,2.0\\\\n\" > /data/user_b/experiment_003/scan_01.csv\n",
    "'\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/\n",
      "\u251c\u2500\u2500 experiment_001.csv\n",
      "\u251c\u2500\u2500 experiment_002.csv\n",
      "\u251c\u2500\u2500 scratch.tmp\n",
      "\u251c\u2500\u2500 user_a\n",
      "\u2502\u00a0\u00a0 \u251c\u2500\u2500 experiment_001\n",
      "\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 scan_01.csv\n",
      "\u2502\u00a0\u00a0 \u2514\u2500\u2500 experiment_002\n",
      "\u2502\u00a0\u00a0     \u2514\u2500\u2500 scan_01.csv\n",
      "\u2514\u2500\u2500 user_b\n",
      "    \u2514\u2500\u2500 experiment_003\n",
      "        \u2514\u2500\u2500 scan_01.csv\n",
      "\n",
      "5 directories, 6 files\n"
     ]
    }
   ],
   "source": [
    "# show current file listing on the \"microscope-01\" simulated instrument\n",
    "_ = run(f\"docker compose -f {SIMLAB_COMPOSE} exec microscope-01 tree /data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9a. Clear and re-harvest to see metadata enrichment\n",
    "\n",
    "We remove all transferred files and file transfer records to simulate harvesting all files again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DELETE 3\n",
      "DELETE 3\n",
      "{\n",
      "  \"flow_run_id\": \"81550980-c8bc-4adc-92f7-92a9b45d0156\",\n",
      "  \"schedule_id\": \"725fa608-f4da-40f5-980a-2aa977ffe7e3\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "_ = run(\"docker compose exec worker rm -rf /storage/*\")\n",
    "_ = run('docker compose exec postgres psql -U streamweave -c \"DELETE FROM file_transfers; DELETE FROM file_records;\"')\n",
    "\n",
    "resp = client.post(f\"/api/schedules/{SCHEDULE_ID}/trigger\", headers=AUTH)\n",
    "METADATA_FLOW_RUN_ID = resp.json().get(\"flow_run_id\")\n",
    "pp(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9b. Check enriched metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for flow run 81550980-c8bc-4adc-92f7-92a9b45d0156 to complete...\n",
      "  State: SCHEDULED (attempt 1/120)\n",
      "  State: SCHEDULED (attempt 2/120)\n",
      "  State: SCHEDULED (attempt 3/120)\n",
      "  State: SCHEDULED (attempt 4/120)\n",
      "  State: SCHEDULED (attempt 5/120)\n",
      "  State: SCHEDULED (attempt 6/120)\n",
      "  State: SCHEDULED (attempt 7/120)\n",
      "  State: SCHEDULED (attempt 8/120)\n",
      "  State: SCHEDULED (attempt 9/120)\n",
      "  State: SCHEDULED (attempt 10/120)\n",
      "  State: RUNNING (attempt 11/120)\n",
      "Flow run finished with state: COMPLETED\n",
      "{\n",
      "  \"filename\": \"scan_01.csv\",\n",
      "  \"source_path\": \"microscope/user_a/experiment_001/scan_01.csv\",\n",
      "  \"metadata_\": {\n",
      "    \"username\": \"user_a\",\n",
      "    \"experiment\": \"experiment_001\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"filename\": \"scan_01.csv\",\n",
      "  \"source_path\": \"microscope/user_a/experiment_002/scan_01.csv\",\n",
      "  \"metadata_\": {\n",
      "    \"username\": \"user_a\",\n",
      "    \"experiment\": \"experiment_002\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"filename\": \"scan_01.csv\",\n",
      "  \"source_path\": \"microscope/user_b/experiment_003/scan_01.csv\",\n",
      "  \"metadata_\": {\n",
      "    \"username\": \"user_b\",\n",
      "    \"experiment\": \"experiment_003\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"filename\": \"scratch.tmp\",\n",
      "  \"source_path\": \"microscope/scratch.tmp\",\n",
      "  \"metadata_\": {}\n",
      "}\n",
      "{\n",
      "  \"filename\": \"experiment_002.csv\",\n",
      "  \"source_path\": \"microscope/experiment_002.csv\",\n",
      "  \"metadata_\": {}\n",
      "}\n",
      "{\n",
      "  \"filename\": \"experiment_001.csv\",\n",
      "  \"source_path\": \"microscope/experiment_001.csv\",\n",
      "  \"metadata_\": {}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Wait for the harvest to complete\n",
    "wait_for_flow_run(METADATA_FLOW_RUN_ID)\n",
    "\n",
    "resp = client.get(\"/api/files\", headers=AUTH)\n",
    "for f in resp.json():\n",
    "    print(json.dumps({\"filename\": f[\"filename\"], \"source_path\": f[\"source_path\"], \"metadata_\": f.get(\"metadata_\")}, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files under user directories should have enriched metadata like:\n",
    "```json\n",
    "{\n",
    "  \"filename\": \"scan_01.csv\",\n",
    "  \"source_path\": \"user_a/experiment_001/scan_01.csv\",\n",
    "  \"metadata_\": {\n",
    "    \"username\": \"user_a\",\n",
    "    \"experiment\": \"experiment_001\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "Files at the root level (like `experiment_001.csv`) won't match the pattern and will have empty metadata \u2014 that's expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. User-Scoped Access Control Demo\n",
    "\n",
    "Files are private by default. Access is granted explicitly to users, groups, or projects via the `FileAccessGrant` system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 10a. Create a regular (non-admin) user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"e20b41d4-b5b4-4b54-bc10-6d8fd3c72d2c\",\n",
      "  \"email\": \"researcher@test.org\",\n",
      "  \"is_active\": true,\n",
      "  \"is_superuser\": false,\n",
      "  \"is_verified\": false,\n",
      "  \"role\": \"user\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "resp = client.post(\"/auth/register\", json={\n",
    "    \"email\": \"researcher@test.org\",\n",
    "    \"password\": \"testpassword123\",\n",
    "})\n",
    "pp(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10b. Get regular user token and user ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User ID: e20b41d4-b5b4-4b54-bc10-6d8fd3c72d2c\n"
     ]
    }
   ],
   "source": [
    "resp = client.post(\"/auth/jwt/login\", data={\"username\": \"researcher@test.org\", \"password\": \"testpassword123\"})\n",
    "USER_TOKEN = resp.json()[\"access_token\"]\n",
    "USER_AUTH = {\"Authorization\": f\"Bearer {USER_TOKEN}\"}\n",
    "\n",
    "resp = client.get(\"/users/me\", headers=USER_AUTH)\n",
    "USER_ID = resp.json()[\"id\"]\n",
    "print(f\"User ID: {USER_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10c. Verify user sees no files (no access granted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files: []\n",
      "Transfers: []\n"
     ]
    }
   ],
   "source": [
    "resp = client.get(\"/api/files\", headers=USER_AUTH)\n",
    "print(\"Files:\", resp.json())\n",
    "# Expected: []\n",
    "\n",
    "resp = client.get(\"/api/transfers\", headers=USER_AUTH)\n",
    "print(\"Transfers:\", resp.json())\n",
    "# Expected: []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10d. Grant direct user access to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ID: fc38212d-4f71-405c-be81-26aa8f173beb\n",
      "{\n",
      "  \"id\": \"84dbfebb-8032-4e27-8ba9-fb518fc4437f\",\n",
      "  \"file_id\": \"fc38212d-4f71-405c-be81-26aa8f173beb\",\n",
      "  \"grantee_type\": \"user\",\n",
      "  \"grantee_id\": \"e20b41d4-b5b4-4b54-bc10-6d8fd3c72d2c\",\n",
      "  \"granted_at\": \"2026-02-24T17:06:11.583824Z\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Pick a file to grant access to\n",
    "FILE_ID = client.get(\"/api/files\", headers=AUTH).json()[0][\"id\"]\n",
    "print(f\"File ID: {FILE_ID}\")\n",
    "\n",
    "# Grant the user access (admin-only endpoint)\n",
    "resp = client.post(f\"/api/files/{FILE_ID}/access\", headers=AUTH, json={\n",
    "    \"grantee_type\": \"user\",\n",
    "    \"grantee_id\": USER_ID,\n",
    "})\n",
    "pp(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected response:\n",
    "```json\n",
    "{\n",
    "  \"id\": \"<grant-uuid>\",\n",
    "  \"file_id\": \"<file-uuid>\",\n",
    "  \"grantee_type\": \"user\",\n",
    "  \"grantee_id\": \"<user-uuid>\",\n",
    "  \"granted_at\": \"2026-02-23T...\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10e. Verify user now sees the granted file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files visible to user: 1\n",
      "{\n",
      "  \"id\": \"fc38212d-4f71-405c-be81-26aa8f173beb\",\n",
      "  \"persistent_id\": \"ark:/99999/fk4biqspimyync43nspm5elixje3m\",\n",
      "  \"persistent_id_type\": \"ark\",\n",
      "  \"instrument_id\": \"4bca34e9-5309-4d8d-b78c-af9ef64845f8\",\n",
      "  \"source_path\": \"microscope/user_a/experiment_001/scan_01.csv\",\n",
      "  \"filename\": \"scan_01.csv\",\n",
      "  \"size_bytes\": 23,\n",
      "  \"source_mtime\": \"2026-02-24T17:05:59.485000Z\",\n",
      "  \"xxhash\": \"7c0d455ba2126c3d\",\n",
      "  \"sha256\": null,\n",
      "  \"first_discovered_at\": \"2026-02-24T17:06:10.729750Z\",\n",
      "  \"metadata_\": {\n",
      "    \"username\": \"user_a\",\n",
      "    \"experiment\": \"experiment_001\"\n",
      "  },\n",
      "  \"owner_id\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "resp = client.get(\"/api/files\", headers=USER_AUTH)\n",
    "print(f\"Files visible to user: {len(resp.json())}\")\n",
    "# Expected: exactly 1 file\n",
    "\n",
    "resp = client.get(f\"/api/files/{FILE_ID}\", headers=USER_AUTH)\n",
    "pp(resp)\n",
    "# Expected: 200 with full file details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10f. Verify 404 for files without access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"detail\": \"File not found\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "OTHER_FILE = client.get(\"/api/files\", headers=AUTH).json()[1][\"id\"]\n",
    "\n",
    "resp = client.get(f\"/api/files/{OTHER_FILE}\", headers=USER_AUTH)\n",
    "pp(resp)\n",
    "# Expected: {\"detail\": \"File not found\"} (404, not 403 \u2014 avoids leaking existence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10g. List and revoke a grant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"id\": \"84dbfebb-8032-4e27-8ba9-fb518fc4437f\",\n",
      "    \"file_id\": \"fc38212d-4f71-405c-be81-26aa8f173beb\",\n",
      "    \"grantee_type\": \"user\",\n",
      "    \"grantee_id\": \"e20b41d4-b5b4-4b54-bc10-6d8fd3c72d2c\",\n",
      "    \"granted_at\": \"2026-02-24T17:06:11.583824Z\"\n",
      "  }\n",
      "]\n",
      "Delete status: 204\n",
      "{\n",
      "  \"detail\": \"File not found\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# List grants for the file (admin only)\n",
    "resp = client.get(f\"/api/files/{FILE_ID}/access\", headers=AUTH)\n",
    "pp(resp)\n",
    "\n",
    "# Revoke the grant\n",
    "GRANT_ID = resp.json()[0][\"id\"]\n",
    "resp = client.delete(f\"/api/files/{FILE_ID}/access/{GRANT_ID}\", headers=AUTH)\n",
    "print(f\"Delete status: {resp.status_code}\")\n",
    "# Expected: 204\n",
    "\n",
    "# Verify user can no longer see the file\n",
    "resp = client.get(f\"/api/files/{FILE_ID}\", headers=USER_AUTH)\n",
    "pp(resp)\n",
    "# Expected: {\"detail\": \"File not found\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10B. Group-Based Access Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 10B-a. Create a group and add the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group ID: 646512c1-d81b-434a-b798-aeb402cd5dca\n",
      "{\n",
      "  \"group_id\": \"646512c1-d81b-434a-b798-aeb402cd5dca\",\n",
      "  \"user_id\": \"e20b41d4-b5b4-4b54-bc10-6d8fd3c72d2c\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Create group\n",
    "resp = client.post(\"/api/groups\", headers=AUTH, json={\n",
    "    \"name\": \"Lab A Researchers\",\n",
    "    \"description\": \"All researchers in Lab A\",\n",
    "})\n",
    "GROUP_ID = resp.json()[\"id\"]\n",
    "print(f\"Group ID: {GROUP_ID}\")\n",
    "\n",
    "# Add the regular user to the group\n",
    "resp = client.post(f\"/api/groups/{GROUP_ID}/members\", headers=AUTH, json={\"user_id\": USER_ID})\n",
    "pp(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10B-b. Grant the group access to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"ca7145e7-ae7b-4518-8657-58461cfd898f\",\n",
      "  \"file_id\": \"fc38212d-4f71-405c-be81-26aa8f173beb\",\n",
      "  \"grantee_type\": \"group\",\n",
      "  \"grantee_id\": \"646512c1-d81b-434a-b798-aeb402cd5dca\",\n",
      "  \"granted_at\": \"2026-02-24T17:06:11.640838Z\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "resp = client.post(f\"/api/files/{FILE_ID}/access\", headers=AUTH, json={\n",
    "    \"grantee_type\": \"group\",\n",
    "    \"grantee_id\": GROUP_ID,\n",
    "})\n",
    "pp(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10B-c. Verify user sees the file via group membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"fc38212d-4f71-405c-be81-26aa8f173beb\",\n",
      "  \"persistent_id\": \"ark:/99999/fk4biqspimyync43nspm5elixje3m\",\n",
      "  \"persistent_id_type\": \"ark\",\n",
      "  \"instrument_id\": \"4bca34e9-5309-4d8d-b78c-af9ef64845f8\",\n",
      "  \"source_path\": \"microscope/user_a/experiment_001/scan_01.csv\",\n",
      "  \"filename\": \"scan_01.csv\",\n",
      "  \"size_bytes\": 23,\n",
      "  \"source_mtime\": \"2026-02-24T17:05:59.485000Z\",\n",
      "  \"xxhash\": \"7c0d455ba2126c3d\",\n",
      "  \"sha256\": null,\n",
      "  \"first_discovered_at\": \"2026-02-24T17:06:10.729750Z\",\n",
      "  \"metadata_\": {\n",
      "    \"username\": \"user_a\",\n",
      "    \"experiment\": \"experiment_001\"\n",
      "  },\n",
      "  \"owner_id\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "resp = client.get(f\"/api/files/{FILE_ID}\", headers=USER_AUTH)\n",
    "pp(resp)\n",
    "# Expected: 200 \u2014 user can see the file because they're in the granted group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10B-d. Groups CRUD (Create, Read, Update, Delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== All groups ===\n",
      "[\n",
      "  {\n",
      "    \"id\": \"646512c1-d81b-434a-b798-aeb402cd5dca\",\n",
      "    \"name\": \"Lab A Researchers\",\n",
      "    \"description\": \"All researchers in Lab A\",\n",
      "    \"created_at\": \"2026-02-24T17:06:11.627889Z\",\n",
      "    \"updated_at\": \"2026-02-24T17:06:11.627889Z\"\n",
      "  }\n",
      "]\n",
      "\n",
      "=== Group details ===\n",
      "{\n",
      "  \"id\": \"646512c1-d81b-434a-b798-aeb402cd5dca\",\n",
      "  \"name\": \"Lab A Researchers\",\n",
      "  \"description\": \"All researchers in Lab A\",\n",
      "  \"created_at\": \"2026-02-24T17:06:11.627889Z\",\n",
      "  \"updated_at\": \"2026-02-24T17:06:11.627889Z\"\n",
      "}\n",
      "\n",
      "=== Group members ===\n",
      "[\n",
      "  {\n",
      "    \"group_id\": \"646512c1-d81b-434a-b798-aeb402cd5dca\",\n",
      "    \"user_id\": \"e20b41d4-b5b4-4b54-bc10-6d8fd3c72d2c\"\n",
      "  }\n",
      "]\n",
      "\n",
      "=== Update group ===\n",
      "{\n",
      "  \"id\": \"646512c1-d81b-434a-b798-aeb402cd5dca\",\n",
      "  \"name\": \"Lab A Researchers\",\n",
      "  \"description\": \"Updated description\",\n",
      "  \"created_at\": \"2026-02-24T17:06:11.627889Z\",\n",
      "  \"updated_at\": \"2026-02-24T17:06:11.661656Z\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# List groups\n",
    "print(\"=== All groups ===\")\n",
    "pp(client.get(\"/api/groups\", headers=AUTH))\n",
    "\n",
    "# Get group details\n",
    "print(\"\\n=== Group details ===\")\n",
    "pp(client.get(f\"/api/groups/{GROUP_ID}\", headers=AUTH))\n",
    "\n",
    "# List group members\n",
    "print(\"\\n=== Group members ===\")\n",
    "pp(client.get(f\"/api/groups/{GROUP_ID}/members\", headers=AUTH))\n",
    "\n",
    "# Update group\n",
    "print(\"\\n=== Update group ===\")\n",
    "pp(client.patch(f\"/api/groups/{GROUP_ID}\", headers=AUTH, json={\"description\": \"Updated description\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove member status: 204\n",
      "{\n",
      "  \"detail\": \"File not found\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Remove member\n",
    "resp = client.delete(f\"/api/groups/{GROUP_ID}/members/{USER_ID}\", headers=AUTH)\n",
    "print(f\"Remove member status: {resp.status_code}\")\n",
    "# Expected: 204\n",
    "\n",
    "# Verify user lost access (group membership removed)\n",
    "resp = client.get(f\"/api/files/{FILE_ID}\", headers=USER_AUTH)\n",
    "pp(resp)\n",
    "# Expected: {\"detail\": \"File not found\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10C. Project-Based File Access Demo\n",
    "\n",
    "Projects can contain both individual users and entire groups. When a file is granted to a project, all members (direct users + users in member groups) can see it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 10C-a. Create a project with user and group members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID: 7d75d750-a051-4405-88de-ddfaaff5e98e\n",
      "{\n",
      "  \"id\": \"b13ade48-a23e-4547-a0a3-57949ce214cd\",\n",
      "  \"project_id\": \"7d75d750-a051-4405-88de-ddfaaff5e98e\",\n",
      "  \"member_type\": \"group\",\n",
      "  \"member_id\": \"646512c1-d81b-434a-b798-aeb402cd5dca\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Re-add user to the group (removed in previous step)\n",
    "client.post(f\"/api/groups/{GROUP_ID}/members\", headers=AUTH, json={\"user_id\": USER_ID})\n",
    "\n",
    "# Create project\n",
    "resp = client.post(\"/api/projects\", headers=AUTH, json={\n",
    "    \"name\": \"Microscopy Study 2026\",\n",
    "    \"description\": \"Main research project\",\n",
    "})\n",
    "PROJECT_ID = resp.json()[\"id\"]\n",
    "print(f\"Project ID: {PROJECT_ID}\")\n",
    "\n",
    "# Add the group as a project member\n",
    "resp = client.post(f\"/api/projects/{PROJECT_ID}/members\", headers=AUTH, json={\n",
    "    \"member_type\": \"group\",\n",
    "    \"member_id\": GROUP_ID,\n",
    "})\n",
    "pp(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10C-b. Grant the project access to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned up 1 existing grants\n",
      "{\n",
      "  \"id\": \"7d714f34-dbeb-494e-913b-1aa576fe1a86\",\n",
      "  \"file_id\": \"fc38212d-4f71-405c-be81-26aa8f173beb\",\n",
      "  \"grantee_type\": \"project\",\n",
      "  \"grantee_id\": \"7d75d750-a051-4405-88de-ddfaaff5e98e\",\n",
      "  \"granted_at\": \"2026-02-24T17:06:11.719943Z\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Clean up previous grants on the file\n",
    "grants = client.get(f\"/api/files/{FILE_ID}/access\", headers=AUTH).json()\n",
    "for g in grants:\n",
    "    client.delete(f\"/api/files/{FILE_ID}/access/{g['id']}\", headers=AUTH)\n",
    "print(f\"Cleaned up {len(grants)} existing grants\")\n",
    "\n",
    "# Grant project access\n",
    "resp = client.post(f\"/api/files/{FILE_ID}/access\", headers=AUTH, json={\n",
    "    \"grantee_type\": \"project\",\n",
    "    \"grantee_id\": PROJECT_ID,\n",
    "})\n",
    "pp(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10C-c. Verify user sees the file via project \u2192 group \u2192 user chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"fc38212d-4f71-405c-be81-26aa8f173beb\",\n",
      "  \"persistent_id\": \"ark:/99999/fk4biqspimyync43nspm5elixje3m\",\n",
      "  \"persistent_id_type\": \"ark\",\n",
      "  \"instrument_id\": \"4bca34e9-5309-4d8d-b78c-af9ef64845f8\",\n",
      "  \"source_path\": \"microscope/user_a/experiment_001/scan_01.csv\",\n",
      "  \"filename\": \"scan_01.csv\",\n",
      "  \"size_bytes\": 23,\n",
      "  \"source_mtime\": \"2026-02-24T17:05:59.485000Z\",\n",
      "  \"xxhash\": \"7c0d455ba2126c3d\",\n",
      "  \"sha256\": null,\n",
      "  \"first_discovered_at\": \"2026-02-24T17:06:10.729750Z\",\n",
      "  \"metadata_\": {\n",
      "    \"username\": \"user_a\",\n",
      "    \"experiment\": \"experiment_001\"\n",
      "  },\n",
      "  \"owner_id\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "resp = client.get(f\"/api/files/{FILE_ID}\", headers=USER_AUTH)\n",
    "pp(resp)\n",
    "# Expected: 200 \u2014 user can see the file because:\n",
    "#   user \u2208 group \u2192 group \u2208 project \u2192 project has file grant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10C-d. Test direct user membership in projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"48b4470f-f365-4eb4-9a13-526122f9fd62\",\n",
      "  \"email\": \"postdoc@test.org\",\n",
      "  \"is_active\": true,\n",
      "  \"is_superuser\": false,\n",
      "  \"is_verified\": false,\n",
      "  \"role\": \"user\"\n",
      "}\n",
      "Postdoc ID: 48b4470f-f365-4eb4-9a13-526122f9fd62\n",
      "{\n",
      "  \"id\": \"4c14f918-a3e4-437c-a402-35784ca749ee\",\n",
      "  \"project_id\": \"7d75d750-a051-4405-88de-ddfaaff5e98e\",\n",
      "  \"member_type\": \"user\",\n",
      "  \"member_id\": \"48b4470f-f365-4eb4-9a13-526122f9fd62\"\n",
      "}\n",
      "\n",
      "Postdoc file access status: 200\n"
     ]
    }
   ],
   "source": [
    "# Create a second user\n",
    "resp = client.post(\"/auth/register\", json={\"email\": \"postdoc@test.org\", \"password\": \"testpassword123\"})\n",
    "pp(resp)\n",
    "\n",
    "resp = client.post(\"/auth/jwt/login\", data={\"username\": \"postdoc@test.org\", \"password\": \"testpassword123\"})\n",
    "POSTDOC_TOKEN = resp.json()[\"access_token\"]\n",
    "POSTDOC_AUTH = {\"Authorization\": f\"Bearer {POSTDOC_TOKEN}\"}\n",
    "POSTDOC_ID = client.get(\"/users/me\", headers=POSTDOC_AUTH).json()[\"id\"]\n",
    "print(f\"Postdoc ID: {POSTDOC_ID}\")\n",
    "\n",
    "# Add postdoc directly to the project (not via group)\n",
    "resp = client.post(f\"/api/projects/{PROJECT_ID}/members\", headers=AUTH, json={\n",
    "    \"member_type\": \"user\",\n",
    "    \"member_id\": POSTDOC_ID,\n",
    "})\n",
    "pp(resp)\n",
    "\n",
    "# Postdoc can also see the file\n",
    "resp = client.get(f\"/api/files/{FILE_ID}\", headers=POSTDOC_AUTH)\n",
    "print(f\"\\nPostdoc file access status: {resp.status_code}\")\n",
    "# Expected: 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10C-e. Projects CRUD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== All projects ===\n",
      "[\n",
      "  {\n",
      "    \"id\": \"7d75d750-a051-4405-88de-ddfaaff5e98e\",\n",
      "    \"name\": \"Microscopy Study 2026\",\n",
      "    \"description\": \"Main research project\",\n",
      "    \"created_at\": \"2026-02-24T17:06:11.689293Z\",\n",
      "    \"updated_at\": \"2026-02-24T17:06:11.689293Z\"\n",
      "  }\n",
      "]\n",
      "\n",
      "=== Project members ===\n",
      "[\n",
      "  {\n",
      "    \"id\": \"b13ade48-a23e-4547-a0a3-57949ce214cd\",\n",
      "    \"project_id\": \"7d75d750-a051-4405-88de-ddfaaff5e98e\",\n",
      "    \"member_type\": \"group\",\n",
      "    \"member_id\": \"646512c1-d81b-434a-b798-aeb402cd5dca\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"4c14f918-a3e4-437c-a402-35784ca749ee\",\n",
      "    \"project_id\": \"7d75d750-a051-4405-88de-ddfaaff5e98e\",\n",
      "    \"member_type\": \"user\",\n",
      "    \"member_id\": \"48b4470f-f365-4eb4-9a13-526122f9fd62\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# List projects\n",
    "print(\"=== All projects ===\")\n",
    "pp(client.get(\"/api/projects\", headers=AUTH))\n",
    "\n",
    "# List project members\n",
    "print(\"\\n=== Project members ===\")\n",
    "pp(client.get(f\"/api/projects/{PROJECT_ID}/members\", headers=AUTH))\n",
    "# Expected: 2 members (1 group + 1 direct user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove member status: 204\n",
      "{\n",
      "  \"detail\": \"File not found\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Remove postdoc from project\n",
    "resp = client.delete(f\"/api/projects/{PROJECT_ID}/members/{POSTDOC_ID}\", headers=AUTH)\n",
    "print(f\"Remove member status: {resp.status_code}\")\n",
    "# Expected: 204\n",
    "\n",
    "# Postdoc loses access\n",
    "resp = client.get(f\"/api/files/{FILE_ID}\", headers=POSTDOC_AUTH)\n",
    "pp(resp)\n",
    "# Expected: {\"detail\": \"File not found\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10C-f. Non-admin users cannot manage groups/projects/grants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET /api/groups: 403 \u2014 {'detail': 'Admin access required'}\n",
      "GET /api/projects: 403 \u2014 {'detail': 'Admin access required'}\n",
      "GET /api/files/fc38212d-4f71-405c-be81-26aa8f173beb/access: 403 \u2014 {'detail': 'Admin access required'}\n"
     ]
    }
   ],
   "source": [
    "# All of these should return 403\n",
    "for endpoint in [\"/api/groups\", \"/api/projects\", f\"/api/files/{FILE_ID}/access\"]:\n",
    "    resp = client.get(endpoint, headers=USER_AUTH)\n",
    "    print(f\"GET {endpoint}: {resp.status_code} \u2014 {resp.json()}\")\n",
    "# Expected: {\"detail\": \"Admin access required\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 11. File & Transfer API Filtering Demo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 11a. Filter files by instrument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files for instrument 4bca34e9-5309-4d8d-b78c-af9ef64845f8: 6\n"
     ]
    }
   ],
   "source": [
    "INSTRUMENT_ID = instruments[0][\"id\"]\n",
    "\n",
    "resp = client.get(f\"/api/files?instrument_id={INSTRUMENT_ID}\", headers=AUTH)\n",
    "print(f\"Files for instrument {INSTRUMENT_ID}: {len(resp.json())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11b. Filter transfers by file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"id\": \"4bf5abaf-b2ed-412b-87f6-3c7326d80ba9\",\n",
      "    \"file_id\": \"fc38212d-4f71-405c-be81-26aa8f173beb\",\n",
      "    \"storage_location_id\": \"7cb0e261-f511-4ff3-96af-0bd3323d0ba7\",\n",
      "    \"destination_path\": \"/storage/archive/Microscope 01/microscope/user_a/experiment_001/scan_01.csv\",\n",
      "    \"transfer_adapter\": \"rclone\",\n",
      "    \"status\": \"completed\",\n",
      "    \"bytes_transferred\": 23,\n",
      "    \"source_checksum\": null,\n",
      "    \"dest_checksum\": \"7c0d455ba2126c3d\",\n",
      "    \"checksum_verified\": false,\n",
      "    \"started_at\": \"2026-02-24T17:06:10.730384Z\",\n",
      "    \"completed_at\": \"2026-02-24T17:06:10.790206Z\",\n",
      "    \"error_message\": null,\n",
      "    \"prefect_flow_run_id\": null\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "FILE_ID = client.get(\"/api/files\", headers=AUTH).json()[0][\"id\"]\n",
    "\n",
    "resp = client.get(f\"/api/transfers?file_id={FILE_ID}\", headers=AUTH)\n",
    "pp(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11c. Get single file by ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"fc38212d-4f71-405c-be81-26aa8f173beb\",\n",
      "  \"persistent_id\": \"ark:/99999/fk4biqspimyync43nspm5elixje3m\",\n",
      "  \"persistent_id_type\": \"ark\",\n",
      "  \"instrument_id\": \"4bca34e9-5309-4d8d-b78c-af9ef64845f8\",\n",
      "  \"source_path\": \"microscope/user_a/experiment_001/scan_01.csv\",\n",
      "  \"filename\": \"scan_01.csv\",\n",
      "  \"size_bytes\": 23,\n",
      "  \"source_mtime\": \"2026-02-24T17:05:59.485000Z\",\n",
      "  \"xxhash\": \"7c0d455ba2126c3d\",\n",
      "  \"sha256\": null,\n",
      "  \"first_discovered_at\": \"2026-02-24T17:06:10.729750Z\",\n",
      "  \"metadata_\": {\n",
      "    \"username\": \"user_a\",\n",
      "    \"experiment\": \"experiment_001\"\n",
      "  },\n",
      "  \"owner_id\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "resp = client.get(f\"/api/files/{FILE_ID}\", headers=AUTH)\n",
    "pp(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify all fields are present: `persistent_id`, `persistent_id_type`, `source_path`, `filename`, `xxhash`, `first_discovered_at`, `metadata_`.\n",
    "\n",
    "### 11d. Get single transfer by ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"0866d35b-f4f2-492d-a562-2a816fd27da8\",\n",
      "  \"file_id\": \"37bc26ec-ed36-4148-ba35-e27b4b3103ac\",\n",
      "  \"storage_location_id\": \"7cb0e261-f511-4ff3-96af-0bd3323d0ba7\",\n",
      "  \"destination_path\": \"/storage/archive/Microscope 01/microscope/experiment_001.csv\",\n",
      "  \"transfer_adapter\": \"rclone\",\n",
      "  \"status\": \"completed\",\n",
      "  \"bytes_transferred\": 302,\n",
      "  \"source_checksum\": null,\n",
      "  \"dest_checksum\": \"2660dec29d8c3f7b\",\n",
      "  \"checksum_verified\": false,\n",
      "  \"started_at\": \"2026-02-24T17:06:10.399059Z\",\n",
      "  \"completed_at\": \"2026-02-24T17:06:10.458032Z\",\n",
      "  \"error_message\": null,\n",
      "  \"prefect_flow_run_id\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "TRANSFER_ID = client.get(\"/api/transfers\", headers=AUTH).json()[0][\"id\"]\n",
    "\n",
    "resp = client.get(f\"/api/transfers/{TRANSFER_ID}\", headers=AUTH)\n",
    "pp(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Test Schedule CRUD with Prefect Sync\n",
    "\n",
    "### 12a. Create a new schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"f1598bd0-af1d-403d-8713-7431e03e5ec1\",\n",
      "  \"instrument_id\": \"4bca34e9-5309-4d8d-b78c-af9ef64845f8\",\n",
      "  \"default_storage_location_id\": \"7cb0e261-f511-4ff3-96af-0bd3323d0ba7\",\n",
      "  \"cron_expression\": \"0 */6 * * *\",\n",
      "  \"prefect_deployment_id\": \"a28a83b2-4d95-45ba-acd3-6b664c9ba1e7\",\n",
      "  \"enabled\": true,\n",
      "  \"created_at\": \"2026-02-24T17:06:11.923544Z\",\n",
      "  \"updated_at\": \"2026-02-24T17:06:11.925086Z\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "resp = client.post(\"/api/schedules\", headers=AUTH, json={\n",
    "    \"instrument_id\": INSTRUMENT_ID,\n",
    "    \"default_storage_location_id\": STORAGE_ID,\n",
    "    \"cron_expression\": \"0 */6 * * *\",\n",
    "    \"enabled\": True,\n",
    "})\n",
    "pp(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that `prefect_deployment_id` is populated (Prefect deployment was created)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12b. Update the schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '725fa608-f4da-40f5-980a-2aa977ffe7e3',\n",
       "  'instrument_id': '4bca34e9-5309-4d8d-b78c-af9ef64845f8',\n",
       "  'default_storage_location_id': '7cb0e261-f511-4ff3-96af-0bd3323d0ba7',\n",
       "  'cron_expression': '*/15 * * * *',\n",
       "  'prefect_deployment_id': 'a28a83b2-4d95-45ba-acd3-6b664c9ba1e7',\n",
       "  'enabled': True,\n",
       "  'created_at': '2026-02-24T17:05:41.521453Z',\n",
       "  'updated_at': '2026-02-24T17:05:41.523843Z'},\n",
       " {'id': '0293924e-a6de-459c-98c0-6d73303d576b',\n",
       "  'instrument_id': '65186f54-074d-430e-b2ca-c24b4861e990',\n",
       "  'default_storage_location_id': '7cb0e261-f511-4ff3-96af-0bd3323d0ba7',\n",
       "  'cron_expression': '*/15 * * * *',\n",
       "  'prefect_deployment_id': '97ee019b-aa52-43b0-84e9-f472b9951c50',\n",
       "  'enabled': True,\n",
       "  'created_at': '2026-02-24T17:05:42.798288Z',\n",
       "  'updated_at': '2026-02-24T17:05:42.799405Z'},\n",
       " {'id': 'a86e9e55-9518-4b9f-9a24-321f3ea3b428',\n",
       "  'instrument_id': '20bfa6b2-87a4-4d41-9c97-d7f59bd92723',\n",
       "  'default_storage_location_id': '51c43e6b-e524-4fd0-aa92-3b40d8fe7cda',\n",
       "  'cron_expression': '*/15 * * * *',\n",
       "  'prefect_deployment_id': 'decb9a53-cd72-4922-959c-9a955338e70f',\n",
       "  'enabled': True,\n",
       "  'created_at': '2026-02-24T17:05:42.839518Z',\n",
       "  'updated_at': '2026-02-24T17:05:42.840528Z'},\n",
       " {'id': 'f1598bd0-af1d-403d-8713-7431e03e5ec1',\n",
       "  'instrument_id': '4bca34e9-5309-4d8d-b78c-af9ef64845f8',\n",
       "  'default_storage_location_id': '7cb0e261-f511-4ff3-96af-0bd3323d0ba7',\n",
       "  'cron_expression': '0 */6 * * *',\n",
       "  'prefect_deployment_id': 'a28a83b2-4d95-45ba-acd3-6b664c9ba1e7',\n",
       "  'enabled': True,\n",
       "  'created_at': '2026-02-24T17:06:11.923544Z',\n",
       "  'updated_at': '2026-02-24T17:06:11.925086Z'}]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get(\"/api/schedules\", headers=AUTH).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"725fa608-f4da-40f5-980a-2aa977ffe7e3\",\n",
      "  \"instrument_id\": \"4bca34e9-5309-4d8d-b78c-af9ef64845f8\",\n",
      "  \"default_storage_location_id\": \"7cb0e261-f511-4ff3-96af-0bd3323d0ba7\",\n",
      "  \"cron_expression\": \"0 */12 * * *\",\n",
      "  \"prefect_deployment_id\": \"a28a83b2-4d95-45ba-acd3-6b664c9ba1e7\",\n",
      "  \"enabled\": true,\n",
      "  \"created_at\": \"2026-02-24T17:05:41.521453Z\",\n",
      "  \"updated_at\": \"2026-02-24T17:06:12.010835Z\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "NEW_SCHEDULE_ID = client.get(\"/api/schedules\", headers=AUTH).json()[0][\"id\"]\n",
    "\n",
    "resp = client.patch(f\"/api/schedules/{NEW_SCHEDULE_ID}\", headers=AUTH, json={\n",
    "    \"cron_expression\": \"0 */12 * * *\",\n",
    "})\n",
    "pp(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify in Prefect UI that the deployment schedule updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Test Idempotent Discovery\n",
    "\n",
    "Trigger the same harvest twice \u2014 the second run should find zero new files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'725fa608-f4da-40f5-980a-2aa977ffe7e3'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SCHEDULE_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigger 1: {'flow_run_id': '12a6ac99-7067-4e23-a245-692db7d52a42', 'schedule_id': '725fa608-f4da-40f5-980a-2aa977ffe7e3'}\n",
      "Waiting for flow run 12a6ac99-7067-4e23-a245-692db7d52a42 to complete...\n",
      "  State: SCHEDULED (attempt 1/120)\n",
      "  State: SCHEDULED (attempt 2/120)\n",
      "  State: SCHEDULED (attempt 3/120)\n",
      "  State: SCHEDULED (attempt 4/120)\n",
      "  State: SCHEDULED (attempt 5/120)\n",
      "  State: PENDING (attempt 6/120)\n",
      "  State: RUNNING (attempt 7/120)\n",
      "Flow run finished with state: COMPLETED\n",
      "Files before: 6\n",
      "\n",
      "Trigger 2: {'flow_run_id': '836fecca-bbb1-4c9a-8280-bdf253751018', 'schedule_id': '725fa608-f4da-40f5-980a-2aa977ffe7e3'}\n",
      "Waiting for flow run 836fecca-bbb1-4c9a-8280-bdf253751018 to complete...\n",
      "  State: SCHEDULED (attempt 1/120)\n",
      "  State: SCHEDULED (attempt 2/120)\n",
      "  State: SCHEDULED (attempt 3/120)\n",
      "  State: SCHEDULED (attempt 4/120)\n",
      "  State: SCHEDULED (attempt 5/120)\n",
      "  State: SCHEDULED (attempt 6/120)\n",
      "  State: SCHEDULED (attempt 7/120)\n",
      "  State: SCHEDULED (attempt 8/120)\n",
      "  State: SCHEDULED (attempt 9/120)\n",
      "  State: SCHEDULED (attempt 10/120)\n",
      "  State: PENDING (attempt 11/120)\n",
      "Flow run finished with state: COMPLETED\n",
      "Files after: 6\n",
      "\n",
      "PASS: No duplicate files\n"
     ]
    }
   ],
   "source": [
    "# First trigger\n",
    "resp = client.post(f\"/api/schedules/{SCHEDULE_ID}/trigger\", headers=AUTH)\n",
    "FLOW_RUN_ID = resp.json().get(\"flow_run_id\")\n",
    "print(\"Trigger 1:\", resp.json())\n",
    "wait_for_flow_run(FLOW_RUN_ID)\n",
    "\n",
    "before = len(client.get(\"/api/files\", headers=AUTH).json())\n",
    "print(f\"Files before: {before}\\n\")\n",
    "\n",
    "# Second trigger\n",
    "resp = client.post(f\"/api/schedules/{SCHEDULE_ID}/trigger\", headers=AUTH)\n",
    "print(\"Trigger 2:\", resp.json())\n",
    "FLOW_RUN_ID = resp.json().get(\"flow_run_id\")\n",
    "wait_for_flow_run(FLOW_RUN_ID)\n",
    "\n",
    "after = len(client.get(\"/api/files\", headers=AUTH).json())\n",
    "print(f\"Files after: {after}\\n\")\n",
    "\n",
    "if before == after:\n",
    "    print(\"PASS: No duplicate files\")\n",
    "else:\n",
    "    print(\"FAIL: Duplicate files created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Verify ARK Identifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files are assigned unique identifiers ([ARK](https://arks.org/about/), by default) when they are discovered. Future versions of StreamWave will support [handles](https://handle.net/) and [DOIs](https://doi.org) as well for globally persistent identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 6\n",
      "Unique ARKs: 6\n",
      "All start with ark:/99999/fk4: True\n",
      "\n",
      "Sample ARKs:\n",
      "  ark:/99999/fk4biqspimyync43nspm5elixje3m\n",
      "  ark:/99999/fk4dkgmm4ypvzbipe4q2zrkqafbaq\n",
      "  ark:/99999/fk4tspym44rwjgrpjekiuihzr3bqi\n",
      "  ark:/99999/fk422lxsysd3nbppjq6jj5bap7hhm\n",
      "  ark:/99999/fk4uk2tvgse5bghpbzl557rogbriu\n"
     ]
    }
   ],
   "source": [
    "files = client.get(\"/api/files\", headers=AUTH).json()\n",
    "arks = [f[\"persistent_id\"] for f in files]\n",
    "\n",
    "print(f\"Total files: {len(arks)}\")\n",
    "print(f\"Unique ARKs: {len(set(arks))}\")\n",
    "print(f\"All start with ark:/99999/fk4: {all(a.startswith('ark:/99999/fk4') for a in arks)}\")\n",
    "print(\"\\nSample ARKs:\")\n",
    "for ark in arks[:5]:\n",
    "    print(f\"  {ark}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Cleanup\n",
    "\n",
    "Run these in a terminal to tear down everything:\n",
    "\n",
    "```bash\n",
    "docker compose down -v\n",
    "cd simlab && docker compose -f docker-compose.simlab.yml down -v && cd ..\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m Container simlab-spectrometer-01-1 Stopping \n",
      "\u001b[0m\u001b[33m Container simlab-xray-diffraction-01-1 Stopping \n",
      "\u001b[0m\u001b[33m Container simlab-microscope-01-1 Stopping \n",
      "\u001b[0m\u001b[33m Container simlab-microscope-01-1 Stopped \n",
      "\u001b[0m\u001b[33m Container simlab-microscope-01-1 Removing \n",
      "\u001b[0m\u001b[33m Container simlab-spectrometer-01-1 Stopped \n",
      "\u001b[0m\u001b[33m Container simlab-spectrometer-01-1 Removing \n",
      "\u001b[0m\u001b[33m Container simlab-xray-diffraction-01-1 Stopped \n",
      "\u001b[0m\u001b[33m Container simlab-xray-diffraction-01-1 Removing \n",
      "\u001b[0m\u001b[33m Container simlab-microscope-01-1 Removed \n",
      "\u001b[0m\u001b[33m Container simlab-spectrometer-01-1 Removed \n",
      "\u001b[0m\u001b[33m Container simlab-xray-diffraction-01-1 Removed \n",
      "\u001b[0m\u001b[33m Network streamweave-simlab Removing \n",
      "\u001b[0m\u001b[33m Network streamweave-simlab Resource is still in use \n",
      "\u001b[0m\u001b[33m Container streamweave-api-1 Stopping \n",
      "\u001b[0m\u001b[33m Container streamweave-worker-1 Stopping \n",
      "\u001b[0m\u001b[33m Container streamweave-worker-1 Stopped \n",
      "\u001b[0m\u001b[33m Container streamweave-worker-1 Removing \n",
      "\u001b[0m\u001b[33m Container streamweave-worker-1 Removed \n",
      "\u001b[0m\u001b[33m Container streamweave-api-1 Stopped \n",
      "\u001b[0m\u001b[33m Container streamweave-api-1 Removing \n",
      "\u001b[0m\u001b[33m Container streamweave-api-1 Removed \n",
      "\u001b[0m\u001b[33m Container streamweave-prefect-server-1 Stopping \n",
      "\u001b[0m\u001b[33m Container streamweave-postgres-1 Stopping \n",
      "\u001b[0m\u001b[33m Container streamweave-postgres-1 Stopped \n",
      "\u001b[0m\u001b[33m Container streamweave-postgres-1 Removing \n",
      "\u001b[0m\u001b[33m Container streamweave-postgres-1 Removed \n",
      "\u001b[0m\u001b[33m Container streamweave-prefect-server-1 Stopped \n",
      "\u001b[0m\u001b[33m Container streamweave-prefect-server-1 Removing \n",
      "\u001b[0m\u001b[33m Container streamweave-prefect-server-1 Removed \n",
      "\u001b[0m\u001b[33m Container streamweave-prefect-postgres-1 Stopping \n",
      "\u001b[0m\u001b[33m Container streamweave-redis-1 Stopping \n",
      "\u001b[0m\u001b[33m Container streamweave-redis-1 Stopped \n",
      "\u001b[0m\u001b[33m Container streamweave-redis-1 Removing \n",
      "\u001b[0m\u001b[33m Container streamweave-redis-1 Removed \n",
      "\u001b[0m\u001b[33m Container streamweave-prefect-postgres-1 Stopped \n",
      "\u001b[0m\u001b[33m Container streamweave-prefect-postgres-1 Removing \n",
      "\u001b[0m\u001b[33m Container streamweave-prefect-postgres-1 Removed \n",
      "\u001b[0m\u001b[33m Volume streamweave_pgdata Removing \n",
      "\u001b[0m\u001b[33m Volume streamweave_prefect_pgdata Removing \n",
      "\u001b[0m\u001b[33m Network streamweave_default Removing \n",
      "\u001b[0m\u001b[33m Volume streamweave_harvest_data Removing \n",
      "\u001b[0m\u001b[33m Volume streamweave_pgdata Removed \n",
      "\u001b[0m\u001b[33m Volume streamweave_prefect_pgdata Removed \n",
      "\u001b[0m\u001b[33m Volume streamweave_harvest_data Removed \n",
      "\u001b[0m\u001b[33m Network streamweave_default Removed \n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# remove additional demo files created in notebook\n",
    "_ = run(f\"docker compose -f {SIMLAB_COMPOSE} exec microscope-01 sh -c 'rm -rf /data/user_a /data/user_b /data/scratch.tmp'\")\n",
    "\n",
    "# teardown \"simlab\" stack\n",
    "_ = run(f\"docker compose -f {SIMLAB_COMPOSE} down -v\")\n",
    "\n",
    "# teardown main streamwave stack\n",
    "_ = run(\"docker compose down -v\")\n",
    "\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm no containers are still running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME      IMAGE     COMMAND   SERVICE   CREATED   STATUS    PORTS\n",
      "NAME      IMAGE     COMMAND   SERVICE   CREATED   STATUS    PORTS\n"
     ]
    }
   ],
   "source": [
    "_ = run(f\"docker compose -f {SIMLAB_COMPOSE} ps\")\n",
    "_ = run(\"docker compose ps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This guide demonstrated the core capabilities of the **StreamWeave** backend, a research data management platform designed to automate the discovery, transfer, and governance of instrument-generated data.\n",
    "\n",
    "### Features Covered\n",
    "\n",
    "- **Automated File Discovery & Transfer** \u2014 Schedules that automatically harvest files from instrument sources, with checksum verification and idempotent processing\n",
    "- **Persistent Identifiers (ARK)** \u2014 Every discovered file receives a unique, standards-compliant ARK identifier for long-term reference\n",
    "- **Workflow Orchestration** \u2014 Prefect-powered flow execution with real-time monitoring, manual triggers, and scheduled runs\n",
    "- **Extensible Hooks System** \u2014 Pre-transfer hooks for filtering files and post-transfer hooks for metadata enrichment\n",
    "- **Fine-Grained Access Control** \u2014 User, group, and project-based permissions with hierarchical inheritance\n",
    "- **Full API Coverage** \u2014 RESTful endpoints for instruments, storage locations, schedules, files, transfers, and access management\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "StreamWeave is ideal for:\n",
    "- Research core facilities managing data from multiple scientific instruments\n",
    "- Laboratories requiring automated data archival with provenance tracking\n",
    "- Organizations needing compliant data governance with audit trails\n",
    "\n",
    "\n",
    "\n",
    "### Interested in deploying StreamWeave for your organization?\n",
    "\n",
    "For deployment assistance, custom integrations, or enterprise support, contact us at:\n",
    "\n",
    "**[https://datasophos.co/#contact](https://datasophos.co/#contact)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
